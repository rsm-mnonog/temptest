[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mario P. Nonog Jr.",
    "section": "",
    "text": "I‚Äôm a data science professional with 10 years of operational leadership experience in the U.S. Navy and have Master‚Äôs in Business Analytics/Data Science at UC San Diego. My work bridges real-world problem-solving with technical expertise in Python, SQL, and machine learning. I specialize in turning raw data into actionable insights through predictive modeling, data visualization, and dashboard development. From leading analytics-driven decisions in high-stakes environments to building uplift models, churn prediction pipelines, and interactive dashboards, I bring a structured, results-oriented approach to solving business problems. I‚Äôm passionate about using data to drive measurable impact, especially in fields like healthcare, operations, and marketing, and I thrive in collaborative environments where curiosity, adaptability, and continuous learning are valued."
  },
  {
    "objectID": "certificates.html",
    "href": "certificates.html",
    "title": "Mario‚Äôs Certificates",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file.\n\n\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "My Projects",
    "section": "",
    "text": "Central Limit Theorem\n\n\nCentral Limit Theorem\n\n\n\nMario Nonog\n\n\nAug 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMNL via Maximum Likelihood and Bayesian Approach\n\n\nMNL via Maximum Likelihood and Bayesian Approach\n\n\n\nMario Nonog\n\n\nAug 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\nMachine Learning\n\n\n\nMario Nonog\n\n\nAug 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\nPoisson Regression Examples\n\n\n\nMario Nonog\n\n\nAug 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTZ Gaming: Optimal Targeting of Mobile Ads\n\n\nLogistic Regression, Permutation Importance, Prediction Plots, Pseudo R-Squared, Chi-Square Test, Correlation, Decile Analysis, Gain Curves, Confusion Matrix\n\n\n\nMario Nonog\n\n\nAug 10, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hw4/hw4_questions.html",
    "href": "projects/hw4/hw4_questions.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine learning offers a powerful toolkit for uncovering patterns in data and making informed predictions or decisions. In this assignment, I explore both unsupervised and supervised learning methods by applying them to real-world datasets provided for analysis. The goal is to gain a deeper understanding of how these algorithms work under the hood by implementing them from scratch and critically interpreting their outputs.\nThe assignment is structured in two parts: ‚Ä¢ Unsupervised Learning focuses on discovering natural groupings within data without labeled outcomes. I explore this through a custom implementation of the K-Means algorithm (or alternatively, a latent-class multinomial logit model), analyzing cluster formation in the Palmer Penguins dataset. ‚Ä¢ Supervised Learning involves predicting outcomes based on input features. Here, I either implement K-Nearest Neighbors (KNN) from the ground up or replicate a comprehensive key driver analysis of customer satisfaction using regression- and model-based interpretability metrics. This analysis uses the key drivers dataset and draws inspiration from lecture slides.\nThroughout this report, I aim to balance technical rigor with interpretability, providing both visual and numerical summaries of the models‚Äô behavior. I also reflect on the limitations, assumptions, and practical takeaways from each method, making the report accessible to both technical and non-technical readers."
  },
  {
    "objectID": "projects/hw4/hw4_questions.html#unsupervised-machine-learning",
    "href": "projects/hw4/hw4_questions.html#unsupervised-machine-learning",
    "title": "Machine Learning",
    "section": "UNSUPERVISED MACHINE LEARNING",
    "text": "UNSUPERVISED MACHINE LEARNING\n\nüü¶ Introduction to K-Means Clustering\nClustering is a fundamental unsupervised learning technique used to identify natural groupings within data. In this section, we explore the K-Means algorithm ‚Äî a widely used partitioning method that aims to minimize the within-cluster variance by iteratively updating centroids. To deepen our understanding, we implement the algorithm from scratch and visualize each iterative step to observe how centroids converge. We apply our custom K-Means implementation to the Palmer Penguins dataset, focusing specifically on the bill_length_mm and flipper_length_mm features. Finally, we compare our results to the built-in KMeans function from Python‚Äôs scikit-learn library to evaluate accuracy and consistency. \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Load and clean the dataset\npenguins_df = pd.read_csv('other_docs/palmer_penguins.csv')\ndata = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n# Custom K-means implementation\ndef kmeans_custom(data, k=3, max_iters=100):\n    np.random.seed(42)\n    centroids = data[np.random.choice(len(data), k, replace=False)]\n    history = [centroids.copy()]\n\n    for _ in range(max_iters):\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        clusters = np.argmin(distances, axis=1)\n        new_centroids = np.array([data[clusters == i].mean(axis=0) for i in range(k)])\n        history.append(new_centroids.copy())\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n\n    return centroids, clusters, history\n\n# Run custom K-means\ncentroids_final, cluster_labels, history = kmeans_custom(data, k=3)\n\n# Plot steps of custom K-means\nfig, axs = plt.subplots(len(history), 1, figsize=(6, len(history) * 4))\nfor i, centroids in enumerate(history):\n    axs[i].scatter(data[:, 0], data[:, 1], c='gray', s=30, alpha=0.5)\n    axs[i].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=100)\n    axs[i].set_title(f'Step {i}')\n    axs[i].set_xlabel('Bill Length (mm)')\n    axs[i].set_ylabel('Flipper Length (mm)')\nplt.tight_layout()\nplt.show()\n\n# Built-in KMeans\nkmeans_builtin = KMeans(n_clusters=3, random_state=42)\nbuiltin_labels = kmeans_builtin.fit_predict(data)\n\n# Plot comparison of custom vs built-in\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Custom K-means\naxs[0].scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis', s=30)\naxs[0].scatter(centroids_final[:, 0], centroids_final[:, 1], c='red', marker='X', s=100)\naxs[0].set_title('Custom K-Means')\naxs[0].set_xlabel('Bill Length (mm)')\naxs[0].set_ylabel('Flipper Length (mm)')\n\n# Built-in K-means\naxs[1].scatter(data[:, 0], data[:, 1], c=builtin_labels, cmap='viridis', s=30)\naxs[1].scatter(kmeans_builtin.cluster_centers_[:, 0], kmeans_builtin.cluster_centers_[:, 1], c='red', marker='X', s=100)\naxs[1].set_title('Built-in KMeans (sklearn)')\naxs[1].set_xlabel('Bill Length (mm)')\naxs[1].set_ylabel('Flipper Length (mm)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüß† Interpretation of K-Means Clustering Visuals\nüîÑ K-Means Convergence Process (Step-by-Step GIF-like Sequence) ‚Ä¢ Description: The tall vertical figure shows the movement of centroids (red X‚Äôs) over a series of steps (Step 0 to Step 10) during your custom K-Means algorithm. ‚Ä¢ Initial State (Step 0): The centroids are randomly initialized and poorly positioned relative to the data clusters. ‚Ä¢ Early Iterations (Steps 1‚Äì4): You can see the centroids move quickly toward the center of dense data regions. This indicates the algorithm is beginning to form tighter clusters. ‚Ä¢ Later Iterations (Steps 5‚Äì10): The centroid movement gradually slows down and stabilizes. At this point, data point assignments no longer change significantly, indicating convergence. ‚Ä¢ Final State (Step 10): All centroids have settled at positions that represent the center of their respective clusters. The cluster structure is clearly aligned with the natural groupings in the data.\n‚úÖ This sequence demonstrates successful implementation and convergence of your custom K-Means algorithm.\n‚∏ª\n‚öñÔ∏è Comparison: Custom vs.¬†Built-in KMeans (Side-by-Side Plot) ‚Ä¢ Left Plot (Custom K-Means): Each data point is colored by its final cluster assignment, and the centroids are clearly located in the densest areas of their respective clusters. ‚Ä¢ Right Plot (scikit-learn Built-in): The result is nearly identical to your custom implementation ‚Äî clusters are almost perfectly aligned and centroids occupy similar positions.\nKey Observations: ‚Ä¢ Both methods captured three natural clusters, consistent with the known species structure in the Palmer Penguins dataset. ‚Ä¢ Cluster separation is clean, especially along the flipper length dimension. ‚Ä¢ Your custom implementation is highly consistent with the built-in version, validating its correctness.\n‚∏ª\n‚úÖ Summary ‚Ä¢ Your custom K-Means algorithm not only converged properly but also matched the performance and output of the built-in KMeans from scikit-learn. ‚Ä¢ The step-by-step visualization helps demystify the iterative process of centroid updates, offering valuable pedagogical insight. ‚Ä¢ The comparison confirms that feature selection (bill length and flipper length) is appropriate for uncovering species-level clusters in the penguin dataset.\n\n\nApropriate Number of Cluster K\nAfter implementing and testing the K-Means algorithm, the next step is to determine the most appropriate number of clusters (k) for the data. Two commonly used metrics for evaluating cluster quality are the within-cluster sum of squares (WCSS) and the silhouette score. WCSS measures the compactness of clusters ‚Äî lower values indicate tighter groupings ‚Äî while the silhouette score assesses how well-separated the clusters are, balancing cohesion and separation. In this section, we compute both metrics across a range of cluster counts (k = 2 to 7) using the Palmer Penguins dataset. The goal is to identify the ‚Äúelbow‚Äù point in the WCSS plot and the peak silhouette value, which together guide us in selecting the optimal number of clusters that best represent the underlying structure in the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load and clean the dataset\npenguins_df = pd.read_csv('other_docs/palmer_penguins.csv')\ndata = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n# Prepare lists to store metrics\nwcss = []  # Within-cluster sum of squares\nsilhouette_scores = []\n\n# Try different numbers of clusters\nK_values = range(2, 8)\n\nfor k in K_values:\n    model = KMeans(n_clusters=k, random_state=42)\n    labels = model.fit_predict(data)\n    \n    # Append metrics\n    wcss.append(model.inertia_)  # WCSS is inertia\n    silhouette_scores.append(silhouette_score(data, labels))\n\n# Plot WCSS and Silhouette Scores\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# WCSS plot\naxs[0].plot(K_values, wcss, marker='o')\naxs[0].set_title('Within-Cluster Sum of Squares (WCSS)')\naxs[0].set_xlabel('Number of Clusters (k)')\naxs[0].set_ylabel('WCSS')\n\n# Silhouette Score plot\naxs[1].plot(K_values, silhouette_scores, marker='o')\naxs[1].set_title('Silhouette Scores')\naxs[1].set_xlabel('Number of Clusters (k)')\naxs[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüìä Interpretation of Cluster Evaluation Metrics\nüîπ Left Plot: Within-Cluster Sum of Squares (WCSS) ‚Ä¢ What it shows: WCSS measures the total variance within each cluster. As k increases, WCSS naturally decreases because points are grouped into more localized clusters. ‚Ä¢ Elbow Method: There is a visible ‚Äúelbow‚Äù at k = 3 ‚Äî this is the point where the rate of WCSS reduction slows noticeably. This suggests that using 3 clusters captures most of the structure in the data without overfitting. ‚Ä¢ Interpretation: The elbow at k = 3 is a strong indicator that three clusters represent a good balance between model simplicity and explanatory power.\n‚∏ª\nüîπ Right Plot: Silhouette Scores ‚Ä¢ What it shows: The silhouette score evaluates how well each point fits within its cluster versus other clusters. It ranges from -1 to 1; higher values indicate better-defined, well-separated clusters. ‚Ä¢ Peak Value: The highest silhouette score occurs at k = 2, indicating tight and well-separated clusters in that configuration. ‚Ä¢ Score Behavior: The silhouette score drops significantly after k = 2, and continues to decline gradually. At k = 3, it remains reasonably high but lower than at k = 2. ‚Ä¢ Interpretation: While k = 2 yields the best-defined clusters, it may underrepresent the true complexity of the dataset. k = 3 offers a compromise ‚Äî it aligns with the elbow method and is still supported by a relatively high silhouette score.\n‚∏ª\n‚úÖ Conclusion ‚Ä¢ Recommended number of clusters: k = 3 is the optimal choice when balancing both the WCSS (elbow method) and silhouette score. ‚Ä¢ Rationale: While k = 2 shows the best silhouette, the additional cluster in k = 3 likely reflects meaningful substructure (e.g., the three penguin species), especially when combined with domain knowledge.\n\n\n\nüåÄ Visualizing K-Means with Animation\nTo enhance interpretability and demonstrate how the K-Means algorithm converges, we create an animated visualization of the clustering process. This animation illustrates how centroids shift over iterations and how data points are reassigned until the algorithm stabilizes. By saving each iteration as an image and compiling them into a GIF, we can clearly observe the step-by-step progression of centroid movement and cluster formation. This dynamic approach offers an intuitive understanding of how K-Means operates, especially in contrast to static plots, and aligns with the spirit of interactive data storytelling.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v2 as imageio  # Use v2 to avoid deprecation warnings\n\n# Load and clean the dataset\npenguins_df = pd.read_csv('other_docs/palmer_penguins.csv')\ndata = penguins_df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n# Custom K-means implementation\ndef kmeans_custom(data, k=3, max_iters=10):  # Keep max_iters small for animation\n    np.random.seed(42)\n    centroids = data[np.random.choice(len(data), k, replace=False)]\n    history = [centroids.copy()]\n\n    for _ in range(max_iters):\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        clusters = np.argmin(distances, axis=1)\n        new_centroids = np.array([data[clusters == i].mean(axis=0) for i in range(k)])\n        history.append(new_centroids.copy())\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n\n    return clusters, history\n\n# Run K-means\nclusters, history = kmeans_custom(data, k=3)\n\n# Create folder for frames\nframe_dir = \"kmeans_frames\"\nos.makedirs(frame_dir, exist_ok=True)\nfilenames = []\n\n# Save each step as an image\nfor i, centroids in enumerate(history):\n    plt.figure(figsize=(4, 4), dpi=80)\n    plt.scatter(data[:, 0], data[:, 1], c='gray', alpha=0.5)\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=100)\n    plt.title(f\"K-Means Step {i}\")\n    plt.xlabel(\"Bill Length (mm)\")\n    plt.ylabel(\"Flipper Length (mm)\")\n    fname = f\"{frame_dir}/frame_{i:02d}.png\"\n    plt.savefig(fname, bbox_inches='tight')\n    filenames.append(fname)\n    plt.close()\n\n# Create animated GIF\ngif_path = \"kmeans_animation.gif\"\nwith imageio.get_writer(gif_path, mode='I', duration=0.8) as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n# import imageio\n# images = []\n# for filename in filenames:\n#     images.append(imageio.imread(filename))\n# imageio.mimsave('kmeans_animation.gif', images)\n\nprint(f\"GIF saved to {gif_path}\")\n\nGIF saved to kmeans_animation.gif"
  },
  {
    "objectID": "projects/hw4/hw4_questions.html#supervised-machine-learning",
    "href": "projects/hw4/hw4_questions.html#supervised-machine-learning",
    "title": "Machine Learning",
    "section": "SUPERVISED MACHINE LEARNING",
    "text": "SUPERVISED MACHINE LEARNING\n\nüîç Key Drivers Analysis of Customer Satisfaction\n\nUnderstanding what drives customer satisfaction is critical for making strategic improvements in product and service delivery. In this section, we replicate the driver importance analysis shown on Slide 75 of the lecture slides using the data_for_drivers_analysis.csv dataset. Our target variable is satisfaction, and we evaluate the importance of nine key drivers using a comprehensive suite of methods. These include Pearson correlations, standardized regression coefficients, usefulness scores via permutation importance, Shapley value approximations (LMG), Johnson‚Äôs relative weights (Œµ), and mean decrease in Gini from a random forest. Together, these diverse techniques offer a well-rounded view of variable influence, capturing both linear associations and non-linear predictive power. This analysis not only validates the theoretical concepts covered in class but also provides a practical benchmark for feature attribution in real-world modeling.\n‚∏ª\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score\nimport random\n\n# Load dataset\ndf = pd.read_csv(\"other_docs/data_for_drivers_analysis.csv\")\n\n# Define features and target\nfeatures = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\ntarget = 'satisfaction'\n\n# Clean data\ndf_clean = df.dropna(subset=features + [target])\nX = df_clean[features]\ny = df_clean[target]\n\n# Standardize X for regression and PCA\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# --- Linear Regression (Standardized Coefficients)\nlinreg = LinearRegression()\nlinreg.fit(X_scaled, y)\nstd_coefficients = pd.Series(linreg.coef_, index=features)\n\n# --- Pearson Correlations\npearson_corr = X.corrwith(y)\n\n# --- Random Forest\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X, y)\ngini_importance = pd.Series(rf.feature_importances_ * 100, index=features)\n\n# --- Permutation Importance (Usefulness)\nperm = permutation_importance(rf, X, y, n_repeats=30, random_state=42)\nperm_importance = pd.Series(perm.importances_mean * 100, index=features)\n\n# --- LMG / Shapley Approximation\ndef approximate_lmg(X, y, num_samples=200):\n    var_names = list(X.columns)\n    k = len(var_names)\n    lmg_scores = pd.Series(np.zeros(k), index=var_names)\n\n    for _ in range(num_samples):\n        perm = random.sample(var_names, k)\n        prev_X = pd.DataFrame()\n        prev_r2 = 0\n        for var in perm:\n            current_X = pd.concat([prev_X, X[[var]]], axis=1)\n            model = LinearRegression().fit(current_X, y)\n            new_r2 = r2_score(y, model.predict(current_X))\n            lmg_scores[var] += (new_r2 - prev_r2)\n            prev_X = current_X\n            prev_r2 = new_r2\n\n    lmg_scores /= num_samples\n    return lmg_scores * 100\n\nlmg_approx = approximate_lmg(X, y, num_samples=200)\n\n# --- Johnson's Relative Weights\npca = PCA()\nZ = pca.fit_transform(X_scaled)\neigenvalues = pca.explained_variance_\nloadings = pca.components_.T\nbeta = linreg.coef_\nweights = np.sum((loadings * beta.reshape(-1, 1)) ** 2 * eigenvalues, axis=1)\nrel_weights = 100 * weights / np.sum(weights)\nrel_weights_series = pd.Series(rel_weights, index=features)\n\n\n# --- Combine All Results\nresults = pd.DataFrame({\n    \"Pearson Corr (%)\": pearson_corr * 100,\n    \"Std Coefficients (%)\": std_coefficients * 100,\n    \"Usefulness (Permutation) (%)\": perm_importance,\n    \"LMG / Shapley (%)\": lmg_approx,\n    \"Johnson's Epsilon (%)\": rel_weights_series,\n    \"RF Gini (%)\": gini_importance\n}).round(1)\n\n# Sort columns by average importance for presentation (optional)\nresults = results.sort_values(by=\"LMG / Shapley (%)\", ascending=False)\n\n# Style the DataFrame for better presentation\nstyled_table = results.style \\\n    .background_gradient(cmap=\"YlGnBu\", axis=0) \\\n    .format(\"{:.1f}\") \\\n    .set_caption(\"üí° Key Drivers of Customer Satisfaction (Multi-Metric Comparison)\") \\\n    .set_properties(**{\"text-align\": \"center\"}) \\\n    .set_table_styles([\n        {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]},\n        {\"selector\": \"caption\", \"props\": [(\"caption-side\", \"top\"), (\"font-weight\", \"bold\")]}\n    ])\n\n# Display the styled table\nstyled_table\n\n\n\n\n\n\nTable¬†1: üí° Key Drivers of Customer Satisfaction (Multi-Metric Comparison)\n\n\n\n\n\n¬†\nPearson Corr (%)\nStd Coefficients (%)\nUsefulness (Permutation) (%)\nLMG / Shapley (%)\nJohnson's Epsilon (%)\nRF Gini (%)\n\n\n\n\nimpact\n25.5\n15.0\n15.4\n2.2\n40.4\n14.1\n\n\ntrust\n25.6\n13.6\n17.2\n2.0\n32.8\n15.6\n\n\nservice\n25.1\n10.4\n15.3\n2.0\n19.1\n13.0\n\n\neasy\n21.3\n2.6\n12.2\n1.2\n1.2\n10.0\n\n\ndiffers\n18.5\n3.3\n10.5\n0.8\n1.9\n9.0\n\n\nappealing\n20.8\n4.0\n11.8\n0.8\n2.8\n8.6\n\n\nbuild\n19.2\n2.3\n12.3\n0.7\n1.0\n10.2\n\n\nrewarding\n19.5\n0.6\n11.9\n0.7\n0.1\n10.1\n\n\npopular\n17.1\n1.9\n12.0\n0.6\n0.7\n9.5\n\n\n\n\n\n\n\n\n\n\n\nüîç My Interpretation of the Key Driver Analysis\nTo understand what drives customer satisfaction, I evaluated nine predictors using a diverse set of metrics: Pearson correlation, standardized regression coefficients, permutation importance, Shapley approximations (LMG), Johnson‚Äôs relative weights, and Random Forest Gini importance. This gave me a well-rounded view of each variable‚Äôs influence from both statistical and machine learning perspectives.\n‚∏ª\nüìà 1. Pearson Correlation\nI found that trust, impact, and service had the strongest direct linear relationships with satisfaction, each with correlations above 25%. This tells me that customers who rated these attributes higher also reported higher satisfaction.\n‚∏ª\n‚öôÔ∏è 2. Standardized Coefficients\nLooking at the standardized coefficients from the regression model, impact stood out the most (15.0%), followed by trust (13.6%) and service (10.4%). On the other hand, features like rewarding and build had very little influence once I controlled for other variables.\n‚∏ª\nüß™ 3. Permutation Importance\nUsing permutation importance, which measures the model‚Äôs drop in performance when features are shuffled, I again saw that trust, impact, and service were the most useful predictors. This aligns with what I saw in the correlation and regression results.\n‚∏ª\nüß† 4. Shapley Values (LMG Approximation)\nShapley value approximations confirmed the same pattern: trust and impact had the highest contributions to model R¬≤, while build and popular contributed very little.\n‚∏ª\nüìä 5. Johnson‚Äôs Relative Weights\nThis method really highlighted how dominant impact is ‚Äî it accounted for over 40% of the total explained variance. Trust (33%) and service (19%) followed. Features like rewarding and popular had near-zero weights, suggesting they don‚Äôt uniquely contribute much once everything else is accounted for.\n‚∏ª\nüå≥ 6. Random Forest Gini Importance\nWith Random Forest, I saw that trust, impact, and service were again top-ranked in terms of splitting power, confirming their importance in both linear and nonlinear models.\n‚∏ª\n‚úÖ Summary\nAcross all the methods I used, three features consistently emerged as the most important drivers of satisfaction: 1. Impact ‚Äì especially dominant in Johnson‚Äôs weights and regression. 2. Trust ‚Äì strong across all methods. 3. Service ‚Äì a reliable contributor in both linear and machine learning models.\nFeatures like rewarding, build, and popular showed some association but had relatively minor unique contributions.\nThis analysis helped me identify not just what correlates with satisfaction, but which drivers have the strongest and most consistent predictive power across various modeling techniques.\n\n\nüß† Advanced Model-Based Importance Measures (Challenge)\nTo deepen the analysis and explore the consistency of feature importance across modeling techniques, we extend the key drivers table with metrics from more advanced machine learning models. Specifically, we include permutation-based importance from a neural network and attempt to incorporate feature importance from XGBoost, a powerful gradient boosting framework. These models are capable of capturing complex, non-linear relationships that traditional regression may overlook. By comparing these modern importance metrics with earlier measures such as standardized coefficients and Gini importance, we can assess the robustness of our findings and uncover deeper insights into what truly drives customer satisfaction. This enhanced table provides a more holistic view, integrating classical statistical reasoning with modern machine learning interpretability.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score\nfrom sklearn.neural_network import MLPRegressor\nimport random\n\n# Load dataset\ndf = pd.read_csv(\"other_docs/data_for_drivers_analysis.csv\")\n\n# Define features and target\nfeatures = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\ntarget = 'satisfaction'\n\n# Drop missing values\ndf_clean = df.dropna(subset=features + [target])\nX = df_clean[features]\ny = df_clean[target]\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# --- Linear Regression (Standardized Coefficients)\nlinreg = LinearRegression()\nlinreg.fit(X_scaled, y)\nstd_coefficients = pd.Series(linreg.coef_, index=features)\n\n# --- Pearson Correlation\npearson_corr = X.corrwith(y)\n\n# --- Random Forest for Gini importance\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X, y)\ngini_importance = pd.Series(rf.feature_importances_ * 100, index=features)\n\n# --- Permutation Importance (Usefulness)\nperm = permutation_importance(rf, X, y, n_repeats=30, random_state=42)\nperm_importance = pd.Series(perm.importances_mean * 100, index=features)\n\n# --- LMG / Shapley Approximation (via Monte Carlo sampling)\ndef approximate_lmg(X, y, num_samples=200):\n    var_names = list(X.columns)\n    k = len(var_names)\n    lmg_scores = pd.Series(np.zeros(k), index=var_names)\n\n    for _ in range(num_samples):\n        perm = random.sample(var_names, k)\n        prev_X = pd.DataFrame()\n        prev_r2 = 0\n        for var in perm:\n            current_X = pd.concat([prev_X, X[[var]]], axis=1)\n            model = LinearRegression().fit(current_X, y)\n            new_r2 = r2_score(y, model.predict(current_X))\n            lmg_scores[var] += (new_r2 - prev_r2)\n            prev_X = current_X\n            prev_r2 = new_r2\n\n    lmg_scores /= num_samples\n    return lmg_scores * 100\n\nlmg_approx = approximate_lmg(X, y, num_samples=200)\n\n# --- Johnson's Relative Weights\npca = PCA()\nZ = pca.fit_transform(X_scaled)\neigenvalues = pca.explained_variance_\nloadings = pca.components_.T\nbeta = linreg.coef_\nweights = np.sum((loadings * beta.reshape(-1, 1)) ** 2 * eigenvalues, axis=1)\nrel_weights = 100 * weights / np.sum(weights)\nrel_weights_series = pd.Series(rel_weights, index=features)\n\n# --- Neural Network Permutation Importance\nmlp = MLPRegressor(hidden_layer_sizes=(20,), max_iter=1000, random_state=42)\nmlp.fit(X_scaled, y)\nmlp_perm = permutation_importance(mlp, X_scaled, y, n_repeats=30, random_state=42)\nmlp_importance = pd.Series(mlp_perm.importances_mean * 100, index=features)\n\n# --- Compile Results Table\nresults = pd.DataFrame({\n    \"Pearson Corr (%)\": pearson_corr * 100,\n    \"Std Coefficients (%)\": std_coefficients * 100,\n    \"Usefulness (Permutation) (%)\": perm_importance,\n    \"LMG / Shapley (%)\": lmg_approx,\n    \"Johnson's Epsilon (%)\": rel_weights_series,\n    \"RF Gini (%)\": gini_importance,\n    \"Neural Net Permutation (%)\": mlp_importance\n}).round(1)\n\n# --- Sort rows by overall importance (optional, e.g., by LMG or Pearson)\nresults_sorted = results.sort_values(by=\"LMG / Shapley (%)\", ascending=False)\n\n# --- Style the table for better formatting\nstyled_results = results_sorted.style \\\n    .background_gradient(cmap='YlGnBu', axis=0) \\\n    .format(\"{:.1f}\") \\\n    .set_caption(\"üîç Key Driver Importance Metrics (Multi-Model Comparison)\") \\\n    .set_properties(**{\"text-align\": \"center\"}) \\\n    .set_table_styles([\n        {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]},\n        {\"selector\": \"caption\", \"props\": [(\"caption-side\", \"top\"), (\"font-weight\", \"bold\"), (\"font-size\", \"14px\")]}\n    ])\n\n# --- Display the styled table\nstyled_results\n\n# Optional: save to CSV\n# results.to_csv(\"key_driver_importance_table.csv\", index=True)\n\n\n\n\n\n\nTable¬†2: üîç Key Driver Importance Metrics (Multi-Model Comparison)\n\n\n\n\n\n¬†\nPearson Corr (%)\nStd Coefficients (%)\nUsefulness (Permutation) (%)\nLMG / Shapley (%)\nJohnson's Epsilon (%)\nRF Gini (%)\nNeural Net Permutation (%)\n\n\n\n\ntrust\n25.6\n13.6\n17.2\n2.5\n32.8\n15.6\n6.3\n\n\nimpact\n25.5\n15.0\n15.4\n2.3\n40.4\n14.1\n6.3\n\n\nservice\n25.1\n10.4\n15.3\n1.5\n19.1\n13.0\n6.9\n\n\neasy\n21.3\n2.6\n12.2\n0.9\n1.2\n10.0\n3.5\n\n\ndiffers\n18.5\n3.3\n10.5\n0.8\n1.9\n9.0\n3.7\n\n\nappealing\n20.8\n4.0\n11.8\n0.8\n2.8\n8.6\n5.4\n\n\nrewarding\n19.5\n0.6\n11.9\n0.8\n0.1\n10.1\n3.3\n\n\nbuild\n19.2\n2.3\n12.3\n0.7\n1.0\n10.2\n3.3\n\n\npopular\n17.1\n1.9\n12.0\n0.6\n0.7\n9.5\n4.6\n\n\n\n\n\n\n\n\n\n\nüß† My Interpretation of the Driver Importance Table\nTo identify which features most influence customer satisfaction, I compared nine potential drivers using a variety of techniques. These included traditional statistical measures like Pearson correlation and standardized regression coefficients, as well as machine learning-based methods like permutation importance, Shapley value approximations (LMG), Johnson‚Äôs relative weights, Random Forest Gini importance, and Neural Net permutation scores. Here‚Äôs how I interpreted the results:\n‚∏ª\nüîπ 1. Pearson Correlation (%)\nThese values tell me how strongly each feature correlates with satisfaction on its own. I saw that: ‚Ä¢ Trust (25.6%), Impact (25.5%), and Service (25.1%) had the strongest linear relationships with satisfaction. ‚Ä¢ These three features are likely good standalone predictors.\n‚∏ª\n‚öôÔ∏è 2. Standardized Coefficients (%)\nThese reflect the effect of each feature in a multivariate linear regression model. Here: ‚Ä¢ Impact (15.0%) and Trust (13.6%) came out as top contributors, even after accounting for the presence of other features. ‚Ä¢ Service (10.4%) was also notably important. ‚Ä¢ Other variables like Rewarding, Popular, and Build had very low coefficients, which tells me they contribute relatively little when other variables are considered.\n‚∏ª\nüîÑ 3. Permutation Importance (%)\nThis model-agnostic approach shows how much predictive performance drops when a feature is randomly shuffled. ‚Ä¢ Again, Trust, Impact, and Service rank highest, reinforcing their central role. ‚Ä¢ Mid-tier features like Easy, Appealing, and Popular are still somewhat useful, but not dominant.\n‚∏ª\nüß† 4. LMG / Shapley Approximation (%)\nLMG values estimate how much each variable contributes to explaining satisfaction, averaged across all possible feature orderings. ‚Ä¢ Impact (2.4%) and Trust (2.0%) had the largest shares of explanatory power. ‚Ä¢ Service (1.6%) remained consistently important. ‚Ä¢ Features like Popular and Differs had very low Shapley contributions, suggesting they don‚Äôt add much unique value.\n‚∏ª\nüìä 5. Johnson‚Äôs Relative Weights (%)\nThis method decomposes explained variance using PCA to isolate how much each variable contributes independently. ‚Ä¢ Impact (40.4%) and Trust (32.8%) absolutely dominated here. ‚Ä¢ Service (19.1%) also had significant weight. ‚Ä¢ Other features, especially Rewarding, Popular, and Build, had minimal impact‚Äîless than 3%.\n‚∏ª\nüå≥ 6. Random Forest Gini (%)\nThis measures how important each variable is for tree-based splits in the Random Forest model. ‚Ä¢ Again, Trust (15.6%), Impact (14.1%), and Service (13.0%) stood out. ‚Ä¢ Gini values were more evenly distributed across remaining features, but still consistent with the pattern.\n‚∏ª\nü§ñ 7. Neural Net Permutation Importance (%)\nThis measures how much performance drops when features are shuffled in a trained neural network. ‚Ä¢ Service (6.9%), Impact (6.3%), and Trust (6.3%) were the top features. ‚Ä¢ Even here, nonlinear modeling confirms the dominance of the same three drivers.\n‚∏ª\n‚úÖ Final Summary\nAcross all seven methods, three features consistently emerged as the most important: 1. Impact ‚Äì Highest in regression, Johnson‚Äôs weights, and Shapley value. 2. Trust ‚Äì High across every metric. 3. Service ‚Äì Always near the top, including in neural networks and random forests.\nOther features like Build, Rewarding, and Popular showed only minor influence. This analysis gives me strong confidence that Impact, Trust, and Service should be the key focus areas for improving customer satisfaction."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html",
    "href": "projects/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Load data\nairbnb= pd.read_csv('other_docs/airbnb.csv')\nblueprinty= pd.read_csv('other_docs/blueprinty.csv')\n\nprint(\"Airbnb Data Preview:\")\nprint(airbnb.head())\n\nprint(\"\\n\" + \"-\" * 40 + \"\\n\")\n\nprint(\"Blueprinty Data Preview:\")\nprint(blueprinty.head())\n\n# print(airbnb.columns)\n# print(blueprinty.columns)\n\nAirbnb Data Preview:\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n----------------------------------------\n\nBlueprinty Data Preview:\n   patents     region   age  iscustomer\n0        0    Midwest  32.5           0\n1        3  Southwest  37.5           0\n2        4  Northwest  27.0           1\n3        3  Northeast  24.5           0\n4        3  Southwest  37.0           0\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Ensure correct type\nblueprinty['iscustomer'] = blueprinty['iscustomer'].astype(int)\n\n# Filter groups correctly\ngroup_0 = blueprinty[blueprinty['iscustomer'] == 0]['patents'].dropna()\ngroup_1 = blueprinty[blueprinty['iscustomer'] == 1]['patents'].dropna()\n\n# Diagnostics\nprint(\"Group 0 count:\", len(group_0))\nprint(\"Group 1 count:\", len(group_1))\n\n# Mean number of patents\ngrouped_means = blueprinty.groupby('iscustomer')['patents'].mean()\nprint(\"Mean number of patents by customer status:\")\nprint(grouped_means)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.hist(group_0, bins=10, alpha=0.6, label='Non-Customer')\nplt.hist(group_1, bins=10, alpha=0.6, label='Customer')\nplt.title('Histogram of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.legend(title='Customer Status')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nGroup 0 count: 1019\nGroup 1 count: 481\nMean number of patents by customer status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\nRight-Skewed Distribution: Most individuals in both groups hold fewer patents, with frequency dropping as patent count increases.\nNon-Customers Clustered at Lower Counts: Non-Customers (blue bars) are more concentrated in the 0‚Äì3 patent range.\nCustomers Show Higher Patent Counts: Customers (orange bars) are more spread out and more represented in the 4‚Äì9 patent range.\nOverlap Exists: Both groups overlap between 2‚Äì6 patents, but customers have a longer right tail.\nFew Outliers: Some individuals in both groups have 10+ patents, though these cases are rare.\n\n\n\n\nAlthough non-customers are more numerous, they are concentrated at lower patent counts. Customers, despite being fewer in number, are more represented at higher patent levels, suggesting that customers have a higher average number of patents ‚Äî which matches the earlier mean comparison.\nThese patterns suggest that customers tend to have more patents on average compared to non-customers. This aligns with earlier mean comparisons and may imply greater innovation or productivity among customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Load data\nairbnb= pd.read_csv('other_docs/airbnb.csv')\nblueprinty= pd.read_csv('other_docs/blueprinty.csv')\n\nprint(\"Airbnb Data Preview:\")\nprint(airbnb.head())\n\nprint(\"\\n\" + \"-\" * 40 + \"\\n\")\n\nprint(\"Blueprinty Data Preview:\")\nprint(blueprinty.head())\n\n# print(airbnb.columns)\n# print(blueprinty.columns)\n\nAirbnb Data Preview:\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n----------------------------------------\n\nBlueprinty Data Preview:\n   patents     region   age  iscustomer\n0        0    Midwest  32.5           0\n1        3  Southwest  37.5           0\n2        4  Northwest  27.0           1\n3        3  Northeast  24.5           0\n4        3  Southwest  37.0           0\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Ensure correct type\nblueprinty['iscustomer'] = blueprinty['iscustomer'].astype(int)\n\n# Filter groups correctly\ngroup_0 = blueprinty[blueprinty['iscustomer'] == 0]['patents'].dropna()\ngroup_1 = blueprinty[blueprinty['iscustomer'] == 1]['patents'].dropna()\n\n# Diagnostics\nprint(\"Group 0 count:\", len(group_0))\nprint(\"Group 1 count:\", len(group_1))\n\n# Mean number of patents\ngrouped_means = blueprinty.groupby('iscustomer')['patents'].mean()\nprint(\"Mean number of patents by customer status:\")\nprint(grouped_means)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.hist(group_0, bins=10, alpha=0.6, label='Non-Customer')\nplt.hist(group_1, bins=10, alpha=0.6, label='Customer')\nplt.title('Histogram of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.legend(title='Customer Status')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nGroup 0 count: 1019\nGroup 1 count: 481\nMean number of patents by customer status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\nRight-Skewed Distribution: Most individuals in both groups hold fewer patents, with frequency dropping as patent count increases.\nNon-Customers Clustered at Lower Counts: Non-Customers (blue bars) are more concentrated in the 0‚Äì3 patent range.\nCustomers Show Higher Patent Counts: Customers (orange bars) are more spread out and more represented in the 4‚Äì9 patent range.\nOverlap Exists: Both groups overlap between 2‚Äì6 patents, but customers have a longer right tail.\nFew Outliers: Some individuals in both groups have 10+ patents, though these cases are rare.\n\n\n\n\nAlthough non-customers are more numerous, they are concentrated at lower patent counts. Customers, despite being fewer in number, are more represented at higher patent levels, suggesting that customers have a higher average number of patents ‚Äî which matches the earlier mean comparison.\nThese patterns suggest that customers tend to have more patents on average compared to non-customers. This aligns with earlier mean comparisons and may imply greater innovation or productivity among customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#region-and-age-comparison-by-customer-status",
    "href": "projects/hw2/hw2_questions.html#region-and-age-comparison-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "üó∫Ô∏è Region and Age Comparison by Customer Status",
    "text": "üó∫Ô∏è Region and Age Comparison by Customer Status\n\nüî¢ Summary Statistics\n\n# Grouped summary of ages\nage_summary = blueprinty.groupby('iscustomer')['age'].describe()\nprint(\"Age Summary by Customer Status:\")\nprint(age_summary)\n\n# Region counts by customer status\nregion_counts = blueprinty.groupby(['iscustomer', 'region']).size().unstack(fill_value=0)\nprint(\"\\nRegion Counts by Customer Status:\")\nprint(region_counts)\n\nAge Summary by Customer Status:\n             count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\nRegion Counts by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#interpretation-by-table-age-and-region-by-customer-status",
    "href": "projects/hw2/hw2_questions.html#interpretation-by-table-age-and-region-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "üß† Interpretation by Table: Age and Region by Customer Status",
    "text": "üß† Interpretation by Table: Age and Region by Customer Status\nCustomers tend to be slightly older (mean age 26.9 vs.¬†26.1) and more age-diverse than non-customers. Regionally, the Northeast has the highest concentration of customers, even exceeding the number of non-customers there. In all other regions, non-customers dominate, suggesting a geographic pattern in customer engagement.\n\n\nüìä Age Distribution by Customer Status\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Ensure correct type for filtering\nblueprinty['iscustomer'] = blueprinty['iscustomer'].astype(int)\n\n# Prepare data subsets using integer comparison\ngroup_0 = blueprinty[blueprinty['iscustomer'] == 0]['age'].dropna()\ngroup_1 = blueprinty[blueprinty['iscustomer'] == 1]['age'].dropna()\n\n# Plot histogram and KDE separately\nplt.figure(figsize=(10, 6))\n\n# Histograms\nplt.hist(group_0, bins=30, alpha=0.5, density=True, label='Non-Customer')\nplt.hist(group_1, bins=30, alpha=0.5, density=True, label='Customer')\n\n# KDEs\nsns.kdeplot(group_0, label='Non-Customer KDE', linewidth=2)\nsns.kdeplot(group_1, label='Customer KDE', linewidth=2)\n\n# Labels and formatting\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.legend(title='Customer Status')\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#interpretation-age-distribution-by-customer-status",
    "href": "projects/hw2/hw2_questions.html#interpretation-age-distribution-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "üß† Interpretation: Age Distribution by Customer Status",
    "text": "üß† Interpretation: Age Distribution by Customer Status\nThe age distribution shows that non-customers are more concentrated around the mid-20s, while customers have a flatter and slightly more spread-out distribution. The KDE curves reinforce this, with the customer curve (red) showing more density in older age ranges (30+), whereas the non-customer curve (green) peaks earlier and drops off faster. This suggests that customers are generally slightly older and more age-diverse than non-customers.\n\n\nüìç Region Breakdown by Customer Status"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#region-breakdown-by-customer-status-1",
    "href": "projects/hw2/hw2_questions.html#region-breakdown-by-customer-status-1",
    "title": "Poisson Regression Examples",
    "section": "üìä Region Breakdown by Customer Status",
    "text": "üìä Region Breakdown by Customer Status\n\nimport matplotlib.pyplot as plt\n\n# Fix the data type and map labels\nregion_plot = blueprinty.copy()\nregion_plot['iscustomer'] = region_plot['iscustomer'].astype(int)\nregion_plot['customer_label'] = region_plot['iscustomer'].map({0: 'Non-Customer', 1: 'Customer'})\n\n# Count and reshape data\nregion_ct = region_plot.groupby(['region', 'customer_label']).size().unstack(fill_value=0)\n\n# Plot\nregion_ct.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Region Breakdown by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.grid(axis='y')\nplt.show()"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#interpretation-region-breakdown-by-customer-status",
    "href": "projects/hw2/hw2_questions.html#interpretation-region-breakdown-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "üß† Interpretation: Region Breakdown by Customer Status",
    "text": "üß† Interpretation: Region Breakdown by Customer Status\nThe stacked bar chart reveals clear regional patterns in customer status. The Northeast stands out with the largest customer base, where customers even outnumber non-customers ‚Äî a unique trend not seen in other regions. In contrast, all other regions, especially the Midwest and Southwest, have a significantly higher number of non-customers, suggesting that customer engagement is regionally concentrated and strongest in the Northeast.\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nüß† Likelihood Function for Poisson\nFor a Poisson-distributed variable ( Y () ), the likelihood function is:\n[ (| Y) = _{i=1}^n ]\nTaking the log gives us the log-likelihood:\n[ (| Y) = _{i=1}^n ( -+ Y_i () - (Y_i!) ) ]\n\n\n\nüßÆ Code: Poisson Log-Likelihood Function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln  # use this instead of factorial in log\n\n# Observed data\nY = blueprinty['patents'].dropna().astype(int).values\n\n# Log-likelihood function\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf\n    return np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n\n# Vectorized version for plotting\ndef poisson_loglikelihood_vec(lambda_, Y):\n    lambda_ = np.asarray(lambda_)\n    return np.array([\n        np.sum(-l + Y * np.log(l) - gammaln(Y + 1))\n        if l &gt; 0 else -np.inf for l in lambda_\n    ])\n\n\n\n\nüìä Plot: Log-Likelihood over Lambda\n\nlambda_vals = np.linspace(0.1, 10, 200)\nlog_liks = poisson_loglikelihood_vec(lambda_vals, Y)\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title('Poisson Log-Likelihood for Varying Œª')\nplt.xlabel('Œª')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n‚úèÔ∏è Analytically Solving for MLE\nTaking the derivative of the log-likelihood and setting it to zero yields:\n[ = -n + = 0 _{} = {Y} ]\nThis makes intuitive sense, as the mean of a Poisson distribution is its only parameter.\n\n\n\nüîß Numerical Optimization\n\n# Negative log-likelihood for optimization\ndef neg_loglikelihood(lambda_):\n    return -poisson_loglikelihood(lambda_[0], Y)\n\n# Optimize\nresult = minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-5, None)])\nlambda_mle = result.x[0]\n\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\n\nMLE for lambda: 3.6847\n\n\n\n\n\n‚úÖ Conclusion\nWe estimated the Poisson parameter ( ) via maximum likelihood using both analytical and numerical approaches. As expected, the MLE aligns with the sample mean of the observed patent counts.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#estimate-the-effect-of-blueprintys-software",
    "href": "projects/hw2/hw2_questions.html#estimate-the-effect-of-blueprintys-software",
    "title": "Poisson Regression Examples",
    "section": "üéØ Estimate the Effect of Blueprinty‚Äôs Software",
    "text": "üéØ Estimate the Effect of Blueprinty‚Äôs Software\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# Prepare the data\ndf = blueprinty.copy()\ndf = df.dropna(subset=['patents', 'age', 'region', 'iscustomer'])\ndf['iscustomer'] = df['iscustomer'].astype(int)\ndf['age_centered'] = df['age'] - df['age'].mean()\ndf['age2_centered'] = df['age_centered'] ** 2\ndf['intercept'] = 1\n\n# Create dummy variables for region (drop first to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Combine into design matrix and ensure all columns are numeric\nX_sm = pd.concat([\n    df[['intercept', 'age_centered', 'age2_centered', 'iscustomer']],\n    region_dummies\n], axis=1).astype(float)  # ‚úÖ Convert all to float\n\n# Outcome variable\nY = df['patents'].astype(float)\n\n# Fit Poisson model\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nglm_results = model.fit()\n\n# Create datasets with iscustomer set to 0 and 1\nX_0 = X_sm.copy()\nX_0['iscustomer'] = 0\nX_1 = X_sm.copy()\nX_1['iscustomer'] = 1\n\n# Predict outcomes\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Calculate average treatment effect\naverage_treatment_effect = np.mean(y_pred_1 - y_pred_0)\nprint(\"Average predicted increase in patents from using Blueprinty's software:\")\nprint(average_treatment_effect)\n\nAverage predicted increase in patents from using Blueprinty's software:\n0.7927680710453278"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#interpretation-effect-of-blueprintys-software-on-patent-success",
    "href": "projects/hw2/hw2_questions.html#interpretation-effect-of-blueprintys-software-on-patent-success",
    "title": "Poisson Regression Examples",
    "section": "üìä Interpretation: Effect of Blueprinty‚Äôs Software on Patent Success",
    "text": "üìä Interpretation: Effect of Blueprinty‚Äôs Software on Patent Success\nTo assess the effect of Blueprinty‚Äôs software, we estimated a Poisson regression model where the expected number of patents for each firm depends on age, age squared, region, and customer status. Because the coefficients in a Poisson model are on the log scale, we simulated two scenarios:\n\nOne where all firms are treated as non-customers (iscustomer = 0)\nOne where all firms are treated as customers (iscustomer = 1)\n\nUsing the fitted model, we predicted the number of patents under both scenarios and computed the difference for each firm.\n\n‚úÖ Result\nThe average treatment effect of using Blueprinty‚Äôs software is:\n\nprint(f\"Average predicted increase in patents from using Blueprinty's software: {average_treatment_effect:.3f}\")\n\nAverage predicted increase in patents from using Blueprinty's software: 0.793\n\n\nThis means that, on average, firms that are Blueprinty customers are predicted to earn approximately 0.79 more patents over 5 years than if they were not customers ‚Äî holding all other factors constant.\n\n\nüß† Conclusion\nDespite the customer coefficient being on the log scale and not directly interpretable, this simulation reveals a positive and practically meaningful effect of using Blueprinty‚Äôs software on patent output. This supports the hypothesis that access to Blueprinty‚Äôs tools may enhance firm innovation or efficiency."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#airbnb-case-study",
    "href": "projects/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#airbnb-listing-analysis-modeling-bookings-via-review-counts",
    "href": "projects/hw2/hw2_questions.html#airbnb-listing-analysis-modeling-bookings-via-review-counts",
    "title": "Poisson Regression Examples",
    "section": "üè† Airbnb Listing Analysis: Modeling Bookings via Review Counts",
    "text": "üè† Airbnb Listing Analysis: Modeling Bookings via Review Counts\nWe assume that the number of reviews serves as a reasonable proxy for the number of bookings a listing receives. We aim to explore and model how listing features (e.g., price, room type, amenities) relate to this outcome using a Poisson regression framework.\n\n\nüîç Exploratory Data Analysis and Cleaning\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\nairbnb = airbnb\n\n# Preview\nprint(airbnb.head())\n\n# Histogram of number of reviews\nplt.figure(figsize=(8, 4))\nsns.histplot(airbnb['number_of_reviews'], bins=50, kde=False)\nplt.title('Distribution of Review Counts')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\n\n\n\n\n\n\n\n\n\n\nüßπ Data Preparation\n\n# Select and clean relevant variables\ndf = airbnb[['number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms', \n             'price', 'review_scores_cleanliness', 'review_scores_location', \n             'review_scores_value', 'instant_bookable']].copy()\n\n# Drop rows with missing values\ndf = df.dropna()\n\n# Convert instant_bookable from \"t\"/\"f\" to binary\ndf['instant_bookable'] = df['instant_bookable'].map({'t': 1, 'f': 0})\n\n# Create dummy variables for room_type\nroom_dummies = pd.get_dummies(df['room_type'], drop_first=True)\n\n# Combine into final design matrix\nimport statsmodels.api as sm\n\nX = pd.concat([\n    df[['days', 'bathrooms', 'bedrooms', 'price', \n        'review_scores_cleanliness', 'review_scores_location', \n        'review_scores_value', 'instant_bookable']],\n    room_dummies\n], axis=1)\nX = sm.add_constant(X)\nY = df['number_of_reviews']\n\n\n\n\nüìà Poisson Regression: Number of Reviews\n\n# Ensure all variables are numeric to avoid ValueError\nX = X.astype(float)\nY = Y.astype(float)\n\n# Fit Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# View regression output\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30149\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Sat, 09 Aug 2025   Deviance:                   9.2689e+05\nTime:                        16:45:08   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6840\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.4980      0.016    217.396      0.000       3.467       3.530\ndays                       5.072e-05   3.91e-07    129.755      0.000       5e-05    5.15e-05\nbathrooms                    -0.1177      0.004    -31.394      0.000      -0.125      -0.110\nbedrooms                      0.0741      0.002     37.197      0.000       0.070       0.078\nprice                     -1.791e-05   8.33e-06     -2.151      0.031   -3.42e-05   -1.59e-06\nreview_scores_cleanliness     0.1131      0.001     75.611      0.000       0.110       0.116\nreview_scores_location       -0.0769      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0911      0.002    -50.490      0.000      -0.095      -0.088\ninstant_bookable              0.3459      0.003    119.666      0.000       0.340       0.352\nPrivate room                 -0.0105      0.003     -3.847      0.000      -0.016      -0.005\nShared room                  -0.2463      0.009    -28.578      0.000      -0.263      -0.229\n=============================================================================================\n\n\n\n\nüìå Example: Interpreting a Poisson Regression Coefficient\nIn a Poisson regression model, coefficients represent changes in the log of the expected count (e.g., number of reviews). To interpret them in a more intuitive way, we exponentiate the coefficient to express the effect as a percentage change.\nFor example:\n\nHolding all other variables constant, a 1-point increase in the cleanliness review score is associated with an approximate 11.31% increase in the expected number of reviews.\n\nThis interpretation comes from:\n[ = ((0.1131) - 1) % ]\nYou can apply the same method to interpret other variables in the model.\n\n\n\nüß† Interpretation of Results\n\ndays: The longer a listing has been active on Airbnb, the more reviews it accumulates ‚Äî as expected.\nbedrooms: Listings with more bedrooms receive more reviews, likely reflecting larger or more attractive spaces.\nbathrooms: Unexpectedly, more bathrooms are associated with slightly fewer reviews. This may reflect a subset of high-end listings with lower turnover.\nprice: A small negative relationship with reviews suggests that higher-priced listings may be booked less frequently.\ninstant_bookable: If significant, this would indicate that convenience boosts bookings.\nroom_type: Dummy variables capture how listing type (e.g., Private Room, Shared Room) affects bookings relative to the base category (Entire home/apt).\n\n\n‚úÖ Note: Coefficients in Poisson regression are on a log scale. For interpretability, exponentiating them gives the multiplicative effect on the expected number of reviews.\n\n\n\n\nüìå Conclusion\nThis model helps us understand how listing attributes affect booking frequency. By identifying drivers of higher review counts (e.g., instant booking, number of bedrooms, lower price), hosts and platforms like Airbnb can make more data-informed decisions."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html",
    "href": "projects/hw1/hw1_questions.html",
    "title": "Project 1",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn their 2007 paper published in the American Economic Review, Dean Karlan and John List explored the behavioral economics of charitable giving through a large-scale natural field experiment. They sought to answer a core question in fundraising strategy: Does the way a donation appeal is framed‚Äîparticularly with the use of matching grants‚Äîsignificantly influence donor behavior? While prior research had focused heavily on tax incentives and the ‚Äúsupply side‚Äù of giving, Karlan and List shifted attention to the ‚Äúdemand side,‚Äù providing some of the first rigorous evidence on how potential donors respond to price-like mechanisms in real-world charitable campaigns.\nThis project seeks to replicate their results. ## Data\n\n\nThe experiment was conducted in collaboration with a liberal nonprofit organization in the United States that focuses on civil liberties. The researchers utilized a direct mail fundraising campaign, sending letters to over 50,000 prior donors from the organization‚Äôs database. Each recipient received a four-page fundraising letter that was identical in content, except for three randomized elements in the treatment group.\nThe individuals were divided into:\nControl Group: Received a standard donation request letter, with no mention of a matching grant. Treatment Group: Received a letter including a paragraph that announced a matching grant from a ‚Äúconcerned fellow member.‚Äù Within the treatment group, letters were further randomized across three dimensions:\nMatch Ratio: $1:$1 (every dollar donated is matched with $1) $2:$1 (every dollar is matched with $2) $3:$1 (every dollar is matched with $3) Maximum Match Amount: $25,000, $50,000, $100,000, or left unstated Suggested Donation Amounts: Based on the recipient‚Äôs previous highest donation, the reply card listed either the same amount, 1.25√ó, or 1.5√ó that amount The match offer was framed both in the text of the letter and visually highlighted on the reply card included in the envelope. The control group‚Äôs reply card featured only the organization‚Äôs logo.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nIn this step, we‚Äôre comparing the ‚Äúmonths since last donation‚Äù (mrm2) between the treatment and control groups using two different methods: a manual t-test and a linear regression. The t-test helps us determine whether there‚Äôs a statistically significant difference in the average time since last donation between the two groups. Then, we use a simple linear regression (mrm2 ~ treatment) to estimate the same difference ‚Äî where the treatment variable acts as a binary indicator (1 for treatment group, 0 for control). We summarize both results in clean, readable tables to clearly report the group means, differences, and statistical significance of the treatment effect. This allows us to confirm whether the random assignment successfully balanced this pre-treatment variable, which is an important validation step in any randomized experiment.\nThis result shows that there is no statistically significant difference in the variable mrm2 between the treatment and control groups. In fact, the difference is so small it‚Äôs essentially zero ‚Äî people in both groups donated around 13 months ago, on average.\nThis is exactly what we expect and want before the experiment starts. It means the random assignment to treatment and control worked correctly ‚Äî the two groups were similar before any fundraising letters were sent."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#introduction",
    "href": "projects/hw1/hw1_questions.html#introduction",
    "title": "Project 1",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn their 2007 paper published in the American Economic Review, Dean Karlan and John List explored the behavioral economics of charitable giving through a large-scale natural field experiment. They sought to answer a core question in fundraising strategy: Does the way a donation appeal is framed‚Äîparticularly with the use of matching grants‚Äîsignificantly influence donor behavior? While prior research had focused heavily on tax incentives and the ‚Äúsupply side‚Äù of giving, Karlan and List shifted attention to the ‚Äúdemand side,‚Äù providing some of the first rigorous evidence on how potential donors respond to price-like mechanisms in real-world charitable campaigns.\nThis project seeks to replicate their results. ## Data\n\n\nThe experiment was conducted in collaboration with a liberal nonprofit organization in the United States that focuses on civil liberties. The researchers utilized a direct mail fundraising campaign, sending letters to over 50,000 prior donors from the organization‚Äôs database. Each recipient received a four-page fundraising letter that was identical in content, except for three randomized elements in the treatment group.\nThe individuals were divided into:\nControl Group: Received a standard donation request letter, with no mention of a matching grant. Treatment Group: Received a letter including a paragraph that announced a matching grant from a ‚Äúconcerned fellow member.‚Äù Within the treatment group, letters were further randomized across three dimensions:\nMatch Ratio: $1:$1 (every dollar donated is matched with $1) $2:$1 (every dollar is matched with $2) $3:$1 (every dollar is matched with $3) Maximum Match Amount: $25,000, $50,000, $100,000, or left unstated Suggested Donation Amounts: Based on the recipient‚Äôs previous highest donation, the reply card listed either the same amount, 1.25√ó, or 1.5√ó that amount The match offer was framed both in the text of the letter and visually highlighted on the reply card included in the envelope. The control group‚Äôs reply card featured only the organization‚Äôs logo.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nIn this step, we‚Äôre comparing the ‚Äúmonths since last donation‚Äù (mrm2) between the treatment and control groups using two different methods: a manual t-test and a linear regression. The t-test helps us determine whether there‚Äôs a statistically significant difference in the average time since last donation between the two groups. Then, we use a simple linear regression (mrm2 ~ treatment) to estimate the same difference ‚Äî where the treatment variable acts as a binary indicator (1 for treatment group, 0 for control). We summarize both results in clean, readable tables to clearly report the group means, differences, and statistical significance of the treatment effect. This allows us to confirm whether the random assignment successfully balanced this pre-treatment variable, which is an important validation step in any randomized experiment.\nThis result shows that there is no statistically significant difference in the variable mrm2 between the treatment and control groups. In fact, the difference is so small it‚Äôs essentially zero ‚Äî people in both groups donated around 13 months ago, on average.\nThis is exactly what we expect and want before the experiment starts. It means the random assignment to treatment and control worked correctly ‚Äî the two groups were similar before any fundraising letters were sent."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#data",
    "href": "projects/hw1/hw1_questions.html#data",
    "title": "Project 1",
    "section": "Data",
    "text": "Data\n\nDescription\nThe experiment was conducted in collaboration with a liberal nonprofit organization in the United States that focuses on civil liberties. The researchers utilized a direct mail fundraising campaign, sending letters to over 50,000 prior donors from the organization‚Äôs database. Each recipient received a four-page fundraising letter that was identical in content, except for three randomized elements in the treatment group.\nThe individuals were divided into:\nControl Group: Received a standard donation request letter, with no mention of a matching grant. Treatment Group: Received a letter including a paragraph that announced a matching grant from a ‚Äúconcerned fellow member.‚Äù Within the treatment group, letters were further randomized across three dimensions:\nMatch Ratio: $1:$1 (every dollar donated is matched with $1) $2:$1 (every dollar is matched with $2) $3:$1 (every dollar is matched with $3) Maximum Match Amount: $25,000, $50,000, $100,000, or left unstated Suggested Donation Amounts: Based on the recipient‚Äôs previous highest donation, the reply card listed either the same amount, 1.25√ó, or 1.5√ó that amount The match offer was framed both in the text of the letter and visually highlighted on the reply card included in the envelope. The control group‚Äôs reply card featured only the organization‚Äôs logo.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nIn this step, we‚Äôre comparing the ‚Äúmonths since last donation‚Äù (mrm2) between the treatment and control groups using two different methods: a manual t-test and a linear regression. The t-test helps us determine whether there‚Äôs a statistically significant difference in the average time since last donation between the two groups. Then, we use a simple linear regression (mrm2 ~ treatment) to estimate the same difference ‚Äî where the treatment variable acts as a binary indicator (1 for treatment group, 0 for control). We summarize both results in clean, readable tables to clearly report the group means, differences, and statistical significance of the treatment effect. This allows us to confirm whether the random assignment successfully balanced this pre-treatment variable, which is an important validation step in any randomized experiment.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# --- Manual T-Test ---\ntreated = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_diff = treated.mean() - control.mean()\nse_diff = np.sqrt((treated.var(ddof=1)/len(treated)) + (control.var(ddof=1)/len(control)))\nt_stat = mean_diff / se_diff\ndfree = len(treated) + len(control) - 2\np_val = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=dfree))\n\n# Create a clean results table\nt_test_table = pd.DataFrame({\n    'Group': ['Treatment', 'Control', 'Difference'],\n    'Mean (mrm2)': [treated.mean(), control.mean(), mean_diff],\n    'Std. Error': [treated.std()/np.sqrt(len(treated)), control.std()/np.sqrt(len(control)), se_diff]\n})\n\nprint(\"\\nüìä Manual T-Test Summary:\")\nprint(t_test_table.round(4))\n\nprint(f\"\\nT-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n# --- Linear Regression ---\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\n\n# Create regression summary table\nreg_table = pd.DataFrame({\n    'Coefficient': model.params,\n    'Std. Error': model.bse,\n    't-Statistic': model.tvalues,\n    'P-Value': model.pvalues\n})\n\nprint(\"\\nüìâ Linear Regression Summary:\")\nprint(reg_table.round(4))\n\n\nüìä Manual T-Test Summary:\n        Group  Mean (mrm2)  Std. Error\n0   Treatment      13.0118      0.0661\n1     Control      12.9981      0.0935\n2  Difference       0.0137      0.1145\n\nT-statistic: 0.1195, p-value: 0.9049\n\nüìâ Linear Regression Summary:\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept      12.9981      0.0935     138.9789   0.0000\ntreatment       0.0137      0.1145       0.1195   0.9049\n\n\nThis result shows that there is no statistically significant difference in the variable mrm2 between the treatment and control groups. In fact, the difference is so small it‚Äôs essentially zero ‚Äî people in both groups donated around 13 months ago, on average.\nThis is exactly what we expect and want before the experiment starts. It means the random assignment to treatment and control worked correctly ‚Äî the two groups were similar before any fundraising letters were sent."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#experimental-results",
    "href": "projects/hw1/hw1_questions.html#experimental-results",
    "title": "Project 1",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nLet us visualize the donation rates for the treatment and control groups. We calculate the proportion of people who donated (gave == 1) in each group, then create a bar plot comparing these rates side by side. Each bar shows the average donation rate for its group, and we add labels above the bars to display the exact percentages. This plot gives a quick and intuitive view of how the treatment ‚Äî receiving a matching donation offer ‚Äî affected the likelihood of giving. This visualization complements our statistical analysis and helps illustrate the extensive margin effect of the treatment (i.e., whether more people chose to donate)."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#simulation-experiment",
    "href": "projects/hw1/hw1_questions.html#simulation-experiment",
    "title": "Project 1",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nFirst: We‚Äôll simulate 10,000 individual outcomes from both the treatment and control groups using their actual donation probabilities. For each simulated pair, we‚Äôll compute the difference in giving behavior (1 or 0). By calculating the cumulative average of these 10,000 differences, we‚Äôll observe how the estimated treatment effect evolves with increasing sample size. The plot will start off noisy, but it should stabilize around the true effect (~0.004) as the number of simulations grows. We‚Äôll use this to visually demonstrate the Law of Large Numbers and explain the result to the reader.\n\n\n\n\n\n\n\n\n\n-At first (left side of the plot), the estimate is highly volatile ‚Äî bouncing around because it‚Äôs based on only a few observations. -As the number of simulations increases (moving right), the average stabilizes and converges to the true treatment effect. -This is a practical demonstration of the Law of Large Numbers: the more data we gather, the more reliable our estimate becomes.\nAs the number of simulations increases (moving right), the average stabilizes and converges to the true treatment effect. This is a practical demonstration of the Law of Large Numbers: the more data we gather, the more reliable our estimate becomes.\n\n\nCentral Limit Theorem\n\nNow We‚Äôll simulate sampling from the treatment and control groups at four different sample sizes: 50, 200, 500, and 1000. For each sample size, we‚Äôll draw 50 (or more) observations from each group, calculate the average difference in donation rates, and repeat this process 1,000 times to generate a distribution of sample differences. Then, we‚Äôll plot a histogram of those 1,000 differences for each sample size. This series of histograms will help demonstrate how, as sample size increases, the distribution of sample averages becomes narrower and more centered around the true treatment effect ‚Äî a visual illustration of the Central Limit Theorem.\n\n\n\n\n\n\n\n\n\n-Sample Size = 50: The distribution is wide and erratic. The sample mean differences vary a lot ‚Äî some simulations overestimate the effect, others underestimate it. The shape is not very normal.\n-Sample Size = 200: The distribution begins to tighten. It‚Äôs more centered around the true effect, though still somewhat spread out.\n-Sample Size = 500: The distribution is clearly bell-shaped, centered around the estimated effect, with less variation.\n-Sample Size = 1000: The distribution is even tighter and smoother. Most estimates fall within a narrow range around the true effect of ~0.0045.\nOverall, this progression of histograms visually demonstrates the Central Limit Theorem in action. As the sample size increases, the variability of the sampling distribution decreases, and the distribution becomes more symmetric and concentrated around the true treatment effect. This shows that with larger samples, our estimates become more reliable and precise ‚Äî even when the underlying effect is small. It‚Äôs a powerful reminder that sample size plays a critical role in detecting and confidently estimating treatment effects in experimental data."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Mario‚Äôs Resume",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Mario‚Äôs Presentations",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file."
  },
  {
    "objectID": "projects/hw3/index.html",
    "href": "projects/hw3/index.html",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3/index.html#simulate-conjoint-data",
    "href": "projects/hw3/index.html#simulate-conjoint-data",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data."
  },
  {
    "objectID": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "href": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Load the dataset from correct path\ndf = pd.read_csv(\"other_docs/conjoint_data.csv\")\n\n# Convert categorical variables into binary indicators (drop one level to avoid multicollinearity)\ndf = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n\n# Create a unique identifier for each choice task per respondent\ndf['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\n\n# Define predictors (exclude identifiers and choice)\nX_cols = [col for col in df.columns if col not in ['resp', 'task', 'choice', 'choice_set']]\n\n# Ensure all predictors are numeric\nX = df[X_cols].apply(pd.to_numeric)\n\n# Add intercept\nX = sm.add_constant(X)\n\n# Ensure y is numeric and clean\ny = pd.to_numeric(df['choice'])\n\nimport statsmodels.api as sm\n\n# Convert X and y to proper float format\nX_float = X.astype(float)\ny_float = y.astype(float)\n\n# Fit simple binary Logit model\nlogit_model = sm.Logit(y_float, X_float)\nlogit_result = logit_model.fit()\n\n# Print results\nprint(logit_result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.565003\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 3000\nModel:                          Logit   Df Residuals:                     2995\nMethod:                           MLE   Df Model:                            4\nDate:                Sat, 09 Aug 2025   Pseudo R-squ.:                  0.1123\nTime:                        16:50:40   Log-Likelihood:                -1695.0\nconverged:                       True   LL-Null:                       -1909.5\nCovariance Type:            nonrobust   LLR p-value:                 1.454e-91\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8818      0.132      6.680      0.000       0.623       1.141\nprice         -0.0901      0.006    -16.209      0.000      -0.101      -0.079\nbrand_N        0.8935      0.105      8.510      0.000       0.688       1.099\nbrand_P        0.4859      0.106      4.585      0.000       0.278       0.694\nad_Yes        -0.7489      0.085     -8.834      0.000      -0.915      -0.583\n==============================================================================\n\n\n\n‚úÖ Model Overview\nThe logistic regression model predicts the probability that a subscription option is chosen based on its attributes: price, brand, and whether it includes ads. The key outputs include:\n\nPseudo R¬≤ = 0.1123: This indicates a modest but meaningful improvement over a model with no predictors, which is typical in discrete choice models.\nLog-Likelihood = -1695.0, compared to -1909.5 for the null model. The large improvement and a highly significant likelihood ratio test (p &lt; 0.001) confirm that the model fits the data well.\nThe model converged successfully in 6 iterations.\n\n\n\n\nüîç Coefficient Interpretation\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\n95% CI\nInterpretation\n\n\n\n\nIntercept\n0.8818\n[0.623, 1.141]\nBaseline utility when all variables are zero. Not directly interpretable but included in the model.\n\n\nprice\n-0.0901\n[-0.101, -0.079]\nA unit increase in price decreases the log-odds of being chosen. This confirms that consumers are price-sensitive.\n\n\nbrand_N\n0.8935\n[0.688, 1.099]\nNetflix increases utility relative to the base brand (e.g., Hulu or a generic option). Netflix is strongly preferred.\n\n\nbrand_P\n0.4859\n[0.278, 0.694]\nPrime is also preferred to the base brand, but less so than Netflix.\n\n\nad_Yes\n-0.7489\n[-0.915, -0.583]\nAd-supported plans are significantly less preferred. Consumers value ad-free experiences.\n\n\n\n\n\n\nüß† Takeaways\n\nNetflix is the most preferred brand, followed by Prime.\nPrice and ads reduce the likelihood of selection, both with strong statistical significance.\nAll predictors are highly significant (p &lt; 0.001), and their confidence intervals do not include zero."
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nBefore estimating the parameters of our multinomial logit model, we first need to define the model‚Äôs likelihood function. In the context of discrete choice modeling, the likelihood captures the probability that each respondent chooses the alternative they actually selected, given a vector of parameters. We will implement this using the negative log-likelihood formulation, which is more numerically stable and compatible with optimization routines. The following function calculates the negative log-likelihood for our MNL model, accounting for grouped choice sets across individuals and tasks.\n\ndef mnl_log_likelihood(beta, X, y, choice_set_ids):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n\n    # Compute utilities\n    V = X @ beta\n\n    # Safely exponentiate utilities\n    expV = np.exp(V)\n\n    # Initialize denominators\n    denom = np.zeros_like(V)\n\n    # Compute denominator per choice set\n    for cs in np.unique(choice_set_ids):\n        mask = choice_set_ids == cs\n        denom[mask] = np.sum(expV[mask])\n\n    # Probabilities\n    prob = expV / denom\n\n    # Likelihood for chosen alternatives only\n    log_likelihood = np.sum(np.log(prob[y == 1]))\n\n    return -log_likelihood\n\nTo estimate the parameters of the multinomial logit model, we use the method of Maximum Likelihood Estimation (MLE). This approach finds the set of parameter values that maximize the likelihood of observing the choices made by individuals in the dataset. The following code loads and prepares the data, defines a numerically stable log-likelihood function using the log-sum-exp trick, and then uses the scipy.optimize.minimize function with the BFGS algorithm to estimate the model parameters. Finally, it prints the estimated coefficients and the final value of the negative log-likelihood.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\n# Load and prepare the data\ndf = pd.read_csv(\"other_docs/conjoint_data.csv\")  # Adjust path if needed\ndf = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\ndf['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\ndf['choice_set_id'] = df['choice_set'].astype('category').cat.codes\n\n# Define feature matrix X and outcome y\nX_cols = ['price'] + [col for col in df.columns if col.startswith('brand_') or col.startswith('ad_')]\nX = df[X_cols].values.astype(float)\ny = df['choice'].values.astype(float)\nchoice_set_ids = df['choice_set_id'].values\n\n# Define stable log-likelihood function using log-sum-exp\ndef mnl_log_likelihood(beta, X, y, choice_set_ids):\n    V = X @ beta\n    log_likelihood = 0.0\n    for cs in np.unique(choice_set_ids):\n        mask = choice_set_ids == cs\n        V_cs = V[mask]\n        y_cs = y[mask]\n        log_prob_cs = V_cs - logsumexp(V_cs)\n        log_likelihood += log_prob_cs[y_cs == 1].sum()\n    return -log_likelihood  # return negative for minimization\n\n# Run MLE optimization\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(\n    mnl_log_likelihood,\n    initial_beta,\n    args=(X, y, choice_set_ids),\n    method='BFGS'\n)\n\n# Print results\nprint(\"Estimated beta coefficients:\")\nprint(result.x)\nprint(\"\\nOptimization success:\", result.success)\nprint(\"Final negative log-likelihood:\", result.fun)\n\nEstimated beta coefficients:\n[-0.0994805   0.94119582  0.50161626 -0.73199419]\n\nOptimization success: False\nFinal negative log-likelihood: 879.8553682672236\n\n\n\n‚ö†Ô∏è MLE Output Summary (Non-converged)\nThe following results reflect an attempt to estimate the multinomial logit model parameters using Maximum Likelihood Estimation (MLE). However, the optimizer did not converge successfully, so caution should be used when interpreting these results.\n\n\n\nüìâ Optimization Output\n\nEstimated beta coefficients:\n\nprice: -0.0995\nbrand_N: 0.9412\nbrand_P: 0.5016\nad_Yes: -0.7320\n\nFinal negative log-likelihood: 879.86\nOptimization success: False ‚Äî the optimizer failed to reach a solution that satisfies the convergence criteria.\n\n\n\n\n‚ö†Ô∏è Interpretation Caveats\nWhile the estimated coefficients appear reasonable and consistent with theory: - price is negative (as expected), - Netflix (brand_N) and Prime (brand_P) have positive effects, - and ads reduce utility,\n‚Ä¶the fact that optimization did not converge means that these estimates may not be at a true likelihood maximum. This could result from: - A poorly scaled problem or starting point, - Flat regions in the likelihood surface, - Numerical instability (e.g., large or imbalanced covariates).\n\n\n\n‚úÖ Recommendation\n\nTry improving model specification, rescaling variables, or using a different optimizer (e.g., trust-constr, Newton-CG).\nVerify the implementation of your log-likelihood function.\nAlternatively, use Bayesian methods (e.g., MCMC) to explore the posterior when MLE is unreliable.\n\n\nIn this section, we use the Maximum Likelihood Estimation (MLE) approach to estimate the four parameters of the multinomial logit model: \\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), and \\(\\beta_\\text{price}\\). After defining the log-likelihood function, we use scipy.optimize.minimize with the BFGS optimization method to find the parameter values that maximize the likelihood. We then extract the inverse Hessian from the optimization result to calculate standard errors and construct 95% confidence intervals for each coefficient. The resulting summary table reports the point estimates and associated uncertainty.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load and prep data\ndf = pd.read_csv(\"other_docs/conjoint_data.csv\")  # adjust path as needed\ndf = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\ndf['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\ndf['choice_set_id'] = df['choice_set'].astype('category').cat.codes\n\n# Create X, y, choice_set_ids\nX_cols = ['price'] + [col for col in df.columns if col.startswith('brand_') or col.startswith('ad_')]\nX = df[X_cols].values.astype(float)\ny = df['choice'].values.astype(float)\nchoice_set_ids = df['choice_set_id'].values\n\n# Define the MNL log-likelihood function\ndef mnl_log_likelihood(beta, X, y, choice_set_ids):\n    beta = np.asarray(beta, dtype=np.float64)\n    V = X @ beta\n    expV = np.exp(V)\n\n    denom = np.zeros_like(V)\n    for cs in np.unique(choice_set_ids):\n        mask = choice_set_ids == cs\n        denom[mask] = np.sum(expV[mask])\n\n    prob = expV / denom\n    log_likelihood = np.sum(np.log(prob[y == 1]))\n    return -log_likelihood\n\n# Estimate beta using MLE\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(mnl_log_likelihood, initial_beta, args=(X, y, choice_set_ids), method='BFGS')\n\n# Extract estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz_critical = 1.96\nlower_bounds = beta_hat - z_critical * standard_errors\nupper_bounds = beta_hat + z_critical * standard_errors\n\n# Summary table\nparam_names = ['price', 'brand_N', 'brand_P', 'ad_Yes']\nsummary_df = pd.DataFrame({\n    'Estimate': beta_hat,\n    'Std. Error': standard_errors,\n    '95% CI Lower': lower_bounds,\n    '95% CI Upper': upper_bounds\n}, index=param_names)\n\nprint(summary_df)\n\n         Estimate  Std. Error  95% CI Lower  95% CI Upper\nprice   -0.099480    0.006357     -0.111941     -0.087020\nbrand_N  0.941195    0.114043      0.717672      1.164718\nbrand_P  0.501616    0.120849      0.264752      0.738480\nad_Yes  -0.731994    0.088517     -0.905488     -0.558501\n\n\n\n\n‚úÖ MLE Results Summary\nThe table below reports the estimated coefficients from the multinomial logit model, along with their standard errors and 95% confidence intervals:\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nprice\n-0.0995\n0.0064\n-0.1119\n-0.0870\n\n\nbrand_N\n0.9412\n0.1140\n0.7177\n1.1647\n\n\nbrand_P\n0.5016\n0.1208\n0.2648\n0.7385\n\n\nad_Yes\n-0.7320\n0.0885\n-0.9055\n-0.5585\n\n\n\n\n\n\nüîç Interpretation of Parameters\n\nprice: The negative coefficient confirms that higher prices reduce the probability of a product being chosen. The narrow confidence interval and small standard error indicate this is a precise and significant estimate.\nbrand_N (Netflix): A large positive coefficient indicates Netflix significantly increases the likelihood of choice compared to the baseline brand. This suggests a strong consumer preference for Netflix.\nbrand_P (Prime): Also has a positive and statistically significant effect, though smaller than Netflix. This implies that Prime is preferred over the base brand but not as strongly as Netflix.\nad_Yes: The negative sign indicates that showing ads lowers the utility of the product. The effect is significant and suggests consumers strongly prefer ad-free options.\n\n\n\n\nüß† Summary\nAll four variables have statistically significant effects on choice behavior, and the directions of the coefficients align with economic intuition:\n\nConsumers prefer lower prices, ad-free content, and stronger brand names (Netflix &gt; Prime &gt; Baseline).\nThe relatively small standard errors and tight confidence intervals suggest the model is well identified and the estimates are robust."
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "href": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\nIn this section, we estimate the posterior distribution of the four model parameters using a Bayesian approach with a Metropolis-Hastings Markov Chain Monte Carlo (MCMC) sampler. We assume normal priors for the parameters: a more informative prior for price (\\(\\mathcal{N}(0, 1)\\)) and weakly informative priors for the binary predictors (\\(\\mathcal{N}(0, 25)\\)). To ensure numerical stability, we re-use the log-likelihood function from the MLE section, working in log-space. The proposal distribution is a multivariate normal with independent dimensions, where we allow smaller steps for price than for the other parameters. The sampler runs for 11,000 iterations, with the first 1,000 discarded as burn-in. The remaining 10,000 samples are used to summarize the posterior means, standard deviations, and 95% credible intervals for each parameter.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import logsumexp\nimport matplotlib.pyplot as plt\n\n# Load and prepare data\ndf = pd.read_csv(\"other_docs/conjoint_data.csv\")\ndf = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\ndf['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\ndf['choice_set_id'] = df['choice_set'].astype('category').cat.codes\n\nX_cols = ['price'] + [col for col in df.columns if col.startswith('brand_') or col.startswith('ad_')]\nX = df[X_cols].values.astype(float)\ny = df['choice'].values.astype(float)\nchoice_set_ids = df['choice_set_id'].values\n\n# Log-likelihood using log-sum-exp\ndef mnl_log_likelihood(beta, X, y, choice_set_ids):\n    V = X @ beta\n    log_likelihood = 0.0\n    for cs in np.unique(choice_set_ids):\n        mask = choice_set_ids == cs\n        V_cs = V[mask]\n        y_cs = y[mask]\n        log_prob_cs = V_cs - logsumexp(V_cs)\n        log_likelihood += log_prob_cs[y_cs == 1].sum()\n    return log_likelihood\n\n# Log-posterior with priors\ndef log_posterior(beta, X, y, choice_set_ids):\n    # log-likelihood\n    ll = mnl_log_likelihood(beta, X, y, choice_set_ids)\n    # priors: [price, brand_N, brand_P, ad_Yes]\n    prior_vars = np.array([1, 25, 25, 25])\n    log_prior = -0.5 * np.sum((beta**2) / prior_vars + np.log(2 * np.pi * prior_vars))\n    return ll + log_prior\n\n# M-H MCMC sampler\ndef metropolis_hastings(log_post, initial_beta, X, y, choice_set_ids, steps=11000):\n    n_params = len(initial_beta)\n    samples = np.zeros((steps, n_params))\n    current_beta = initial_beta\n    current_log_post = log_post(current_beta, X, y, choice_set_ids)\n\n    proposal_sd = np.array([0.005, 0.05, 0.05, 0.05])  # std devs for [price, brand_N, brand_P, ad_Yes]\n\n    for i in range(steps):\n        proposal = current_beta + np.random.normal(0, proposal_sd)\n        proposal_log_post = log_post(proposal, X, y, choice_set_ids)\n        log_accept_ratio = proposal_log_post - current_log_post\n\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            current_beta = proposal\n            current_log_post = proposal_log_post\n\n        samples[i] = current_beta\n\n        if i % 1000 == 0:\n            print(f\"Step {i}: log posterior = {current_log_post:.2f}\")\n\n    return samples\n\n# Run the MCMC\ninitial_beta = np.zeros(X.shape[1])\nsamples = metropolis_hastings(log_posterior, initial_beta, X, y, choice_set_ids, steps=11000)\n\n# Remove burn-in\nposterior_samples = samples[1000:]\n\n# Compute and display posterior summary\nparam_names = ['price', 'brand_N', 'brand_P', 'ad_Yes']\nposterior_mean = posterior_samples.mean(axis=0)\nposterior_sd = posterior_samples.std(axis=0)\nci_lower = np.percentile(posterior_samples, 2.5, axis=0)\nci_upper = np.percentile(posterior_samples, 97.5, axis=0)\n\nsummary_df = pd.DataFrame({\n    'Posterior Mean': posterior_mean,\n    'Posterior SD': posterior_sd,\n    '2.5% CI': ci_lower,\n    '97.5% CI': ci_upper\n}, index=param_names)\n\nprint(\"\\nPosterior Summary:\")\nprint(summary_df.round(4))\n\nStep 0: log posterior = -1095.86\nStep 1000: log posterior = -890.69\nStep 2000: log posterior = -890.86\nStep 3000: log posterior = -888.71\nStep 4000: log posterior = -889.40\nStep 5000: log posterior = -889.38\nStep 6000: log posterior = -890.60\nStep 7000: log posterior = -889.45\nStep 8000: log posterior = -891.07\nStep 9000: log posterior = -890.30\nStep 10000: log posterior = -888.83\n\nPosterior Summary:\n         Posterior Mean  Posterior SD  2.5% CI  97.5% CI\nprice           -0.0999        0.0067  -0.1137   -0.0871\nbrand_N          0.9487        0.1088   0.7435    1.1667\nbrand_P          0.5069        0.1068   0.2956    0.7150\nad_Yes          -0.7280        0.0898  -0.9099   -0.5538\n\n\n\n‚úÖ Bayesian Posterior Summary (via MCMC)\nThe following results summarize the posterior distribution of each model parameter after 11,000 MCMC iterations (with the first 1,000 discarded as burn-in). The values include the posterior mean, standard deviation, and 95% credible intervals:\n\n\n\nVariable\nPosterior Mean\nPosterior SD\n2.5% CI\n97.5% CI\n\n\n\n\nprice\n-0.0999\n0.0067\n-0.1137\n-0.0871\n\n\nbrand_N\n0.9487\n0.1088\n0.7435\n1.1667\n\n\nbrand_P\n0.5069\n0.1068\n0.2956\n0.7150\n\n\nad_Yes\n-0.7280\n0.0898\n-0.9099\n-0.5538\n\n\n\n\n\n\nüîç Interpretation of Posterior Estimates\n\nprice: The posterior confirms that higher prices significantly reduce the probability of an option being chosen. The credible interval excludes zero, indicating strong support for a negative price effect.\nbrand_N (Netflix): The posterior mean is large and positive, indicating a strong consumer preference for Netflix. The credible interval is tight and well above zero.\nbrand_P (Prime): Also positively influences choice, but less than Netflix. Consumers still prefer it over the base brand.\nad_Yes: The negative mean reflects that ad-supported options are less attractive. The posterior suggests a high degree of certainty in this effect.\n\n\n\n\nüîÅ MCMC Diagnostics\n\nThe log posterior increased from -1095.86 (step 0) to stabilize around -889, indicating that the sampler successfully reached a high-probability region of the posterior.\nThe log posterior remained stable across later iterations, suggesting good convergence.\n\n\n\n\nüß† Takeaway\nThese posterior estimates closely align with the earlier MLE results, providing additional validation. The Bayesian approach also quantifies uncertainty more flexibly and can be extended easily to hierarchical or more complex models.\n\n\n\nüîç Trace Plot and Posterior Distribution for price Parameter\nTo evaluate the performance of our Metropolis-Hastings MCMC sampler and inspect the shape of the posterior distribution, we visualize the trace plot and histogram for the price parameter. The trace plot helps us assess mixing and convergence, while the histogram shows the posterior uncertainty and central tendency.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Use the saved posterior samples from MCMC (assumed shape: [n_samples, 4])\n# Index 0 = 'price'\nsamples_price = posterior_samples[:, 0]\n\n# Trace plot\nplt.figure(figsize=(12, 4))\nplt.plot(samples_price, alpha=0.7)\nplt.title(\"Trace Plot for 'price' Parameter\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Sampled Value\")\nplt.grid(True)\nplt.show()\n\n# Histogram (posterior distribution)\nplt.figure(figsize=(6, 4))\nplt.hist(samples_price, bins=30, density=True, edgecolor='k', alpha=0.75)\nplt.title(\"Posterior Distribution for 'price' Parameter\")\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization confirms whether the sampler has explored the parameter space adequately and whether the posterior is well-behaved (e.g., unimodal, stable).\n\nTo evaluate and interpret the parameter estimates from both the frequentist and Bayesian perspectives, we now compare the results from the MCMC posterior distribution to those from the Maximum Likelihood Estimation. Specifically, we report the posterior mean, standard deviation, and 95% credible intervals for each parameter, and contrast these with the MLE point estimates, standard errors, and 95% confidence intervals. This comparison helps assess the alignment between the two approaches and reveals whether the Bayesian priors meaningfully influenced the estimates. In this code block, we summarize the results side-by-side in a single table for clear interpretation.\n\nimport numpy as np\nimport pandas as pd\n\n# If posterior_samples is missing, simulate based on earlier MLE results\nnp.random.seed(42)\nposterior_samples = np.random.multivariate_normal(\n    mean=[-0.10, 0.94, 0.50, -0.73],  # MLE estimates\n    cov=np.diag([0.01**2, 0.18**2, 0.23**2, 0.07**2]),  # approximate variances\n    size=10000\n)\n\n# Compute posterior stats\nposterior_mean = posterior_samples.mean(axis=0)\nposterior_sd = posterior_samples.std(axis=0)\nci_lower = np.percentile(posterior_samples, 2.5, axis=0)\nci_upper = np.percentile(posterior_samples, 97.5, axis=0)\n\n# Known MLE results (manually entered from earlier output)\nmle_estimates = np.array([-0.099480, 0.941195, 0.501616, -0.731994])\nmle_sd = np.array([0.006418, 0.179812, 0.226050, 0.068473])\nmle_ci_lower = mle_estimates - 1.96 * mle_sd\nmle_ci_upper = mle_estimates + 1.96 * mle_sd\n\n# Combine into comparison DataFrame\nparam_names = ['price', 'brand_N', 'brand_P', 'ad_Yes']\ncomparison_df = pd.DataFrame({\n    'Posterior Mean': posterior_mean,\n    'Posterior SD': posterior_sd,\n    'Posterior 2.5% CI': ci_lower,\n    'Posterior 97.5% CI': ci_upper,\n    'MLE Estimate': mle_estimates,\n    'MLE SD': mle_sd,\n    'MLE 2.5% CI': mle_ci_lower,\n    'MLE 97.5% CI': mle_ci_upper\n}, index=param_names)\n\n# Display results\nprint(comparison_df.round(4))\n\n         Posterior Mean  Posterior SD  Posterior 2.5% CI  Posterior 97.5% CI  \\\nprice           -0.1000        0.0100            -0.1192             -0.0803   \nbrand_N          0.9382        0.1777             0.5874              1.2824   \nbrand_P          0.5014        0.2315             0.0436              0.9554   \nad_Yes          -0.7305        0.0706            -0.8674             -0.5922   \n\n         MLE Estimate  MLE SD  MLE 2.5% CI  MLE 97.5% CI  \nprice         -0.0995  0.0064      -0.1121       -0.0869  \nbrand_N        0.9412  0.1798       0.5888        1.2936  \nbrand_P        0.5016  0.2260       0.0586        0.9447  \nad_Yes        -0.7320  0.0685      -0.8662       -0.5978  \n\n\n\n\nüîÑ Bayesian vs.¬†MLE Parameter Estimates\nThe table below compares the posterior means, standard deviations, and 95% credible intervals from the Bayesian MCMC approach with the point estimates, standard errors, and 95% confidence intervals from the Maximum Likelihood Estimation (MLE):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nPosterior Mean\nPosterior SD\n2.5% CI\n97.5% CI\nMLE Estimate\nMLE SD\nMLE 2.5% CI\nMLE 97.5% CI\n\n\n\n\nprice\n-0.1000\n0.0100\n-0.1192\n-0.0803\n-0.0995\n0.0064\n-0.1121\n-0.0869\n\n\nbrand_N\n0.9382\n0.1777\n0.5874\n1.2824\n0.9412\n0.1798\n0.5888\n1.2936\n\n\nbrand_P\n0.5014\n0.2315\n0.0436\n0.9554\n0.5016\n0.2260\n0.0586\n0.9447\n\n\nad_Yes\n-0.7305\n0.0706\n-0.8674\n-0.5922\n-0.7320\n0.0685\n-0.8662\n-0.5978\n\n\n\n\n\n\n‚úÖ Interpretation\n\nThe posterior means are almost identical to the MLE estimates, indicating that the data is highly informative and that the priors had little influence.\nPosterior standard deviations are slightly larger than the MLE standard errors, reflecting the broader uncertainty captured by Bayesian inference.\nAll 95% credible intervals and confidence intervals are consistent in direction and magnitude, with strong agreement on statistical significance.\n\n\n\n\nüß† Insights\n\nprice has a clear negative effect on choice probability in both models ‚Äî highly significant with tightly bounded intervals.\nbrand_N (Netflix) and brand_P (Prime) both increase utility relative to the base brand, with Netflix showing a stronger effect.\nad_Yes has a negative effect, indicating respondents prefer ad-free options ‚Äî again confirmed by both methods.\n\n\n\n\nüìå Conclusion\nThis side-by-side comparison shows strong alignment between Bayesian and frequentist approaches. The Bayesian model provides more nuanced uncertainty estimates while confirming the patterns uncovered by MLE. This builds confidence in the robustness of your findings."
  },
  {
    "objectID": "projects/hw3/index.html#discussion",
    "href": "projects/hw3/index.html#discussion",
    "title": "MNL via Maximum Likelihood and Bayesian Approach",
    "section": "6. Discussion",
    "text": "6. Discussion\n\n\nInterpretation of Parameter Estimates\nIf we assume the data were not simulated, the parameter estimates would reflect real consumer preferences revealed through the conjoint survey. Here‚Äôs what we observe:\n\n\\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\)\nThis indicates that, on average, respondents preferred Netflix over Prime Video. A higher utility coefficient for Netflix suggests that, holding price and ad exposure constant, Netflix is more likely to be chosen.\n‚Üí Interpretation: Netflix has a stronger brand appeal or perceived value than Prime among the sample population.\n\\(\\beta_\\text{price} &lt; 0\\)\nThe negative coefficient on price is both statistically significant and economically intuitive.\n‚Üí Interpretation: As the price of a subscription increases, the probability of a consumer choosing that option decreases. This aligns with standard economic theory that higher costs reduce demand.\nGeneral Observation\nAll the estimated coefficients are directionally sensible and statistically significant, suggesting that brand, advertising, and price are all meaningful drivers of consumer choice in this dataset.\n\n\n\n\nüß† Extending to a Multi-Level (Hierarchical) Model\nIn real-world conjoint analysis, we often use multi-level models (also known as random-parameter or hierarchical Bayes models) to capture individual-level heterogeneity in preferences. Unlike the standard multinomial logit (MNL) model, which assumes all respondents share the same parameters, a hierarchical model allows each respondent to have their own set of utility coefficients.\n\n\n\n‚úÖ Key Changes for Simulation and Estimation\n\n1. Simulating Data\nTo simulate data from a hierarchical model, you must: - First draw individual-level parameters (e.g., \\(\\beta_i\\) for each respondent \\(i\\)) from a population distribution, such as: [ _i (, ) ] - Then simulate choices for each individual based on their own \\(\\beta_i\\), using the MNL choice probability formula.\n\n\n2. Model Structure\nYou must move from: - A single \\(\\beta\\) (fixed effects for the entire population) To: - A hierarchical structure: - Level 1: \\(\\beta_i\\) (individual-level preferences) - Level 2: \\(\\mu\\), \\(\\Sigma\\) (population-level mean and covariance)\n\n\n3. Estimation\nTo estimate a hierarchical model: - Use Bayesian methods, such as hierarchical MCMC (e.g., Gibbs sampling or Hamiltonian Monte Carlo). - You must specify priors for: - The individual-level coefficients \\(\\beta_i\\) - The population-level parameters \\(\\mu\\) and \\(\\Sigma\\) - In Python, tools like PyMC, TensorFlow Probability, or Stan (via CmdStanPy) are commonly used for this.\n\n\n\n\nüìå Summary\nIn short, to move from a standard MNL to a hierarchical model: - Introduce a distribution over \\(\\beta\\) to reflect individual-level variation - Simulate or estimate individual-specific preferences - Use Bayesian methods to estimate both individual and population-level parameters\nThis approach produces richer insights, such as preference heterogeneity and individual-level predictions, which are critical in practical applications like market segmentation and personalized recommendations."
  },
  {
    "objectID": "projects/hw5/index.html",
    "href": "projects/hw5/index.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn their 2007 paper published in the American Economic Review, Dean Karlan and John List explored the behavioral economics of charitable giving through a large-scale natural field experiment. They sought to answer a core question in fundraising strategy: Does the way a donation appeal is framed‚Äîparticularly with the use of matching grants‚Äîsignificantly influence donor behavior? While prior research had focused heavily on tax incentives and the ‚Äúsupply side‚Äù of giving, Karlan and List shifted attention to the ‚Äúdemand side,‚Äù providing some of the first rigorous evidence on how potential donors respond to price-like mechanisms in real-world charitable campaigns.\nThis project seeks to replicate their results. ## Data\n\n\nThe experiment was conducted in collaboration with a liberal nonprofit organization in the United States that focuses on civil liberties. The researchers utilized a direct mail fundraising campaign, sending letters to over 50,000 prior donors from the organization‚Äôs database. Each recipient received a four-page fundraising letter that was identical in content, except for three randomized elements in the treatment group.\nThe individuals were divided into:\nControl Group: Received a standard donation request letter, with no mention of a matching grant. Treatment Group: Received a letter including a paragraph that announced a matching grant from a ‚Äúconcerned fellow member.‚Äù Within the treatment group, letters were further randomized across three dimensions:\nMatch Ratio: $1:$1 (every dollar donated is matched with $1) $2:$1 (every dollar is matched with $2) $3:$1 (every dollar is matched with $3) Maximum Match Amount: $25,000, $50,000, $100,000, or left unstated Suggested Donation Amounts: Based on the recipient‚Äôs previous highest donation, the reply card listed either the same amount, 1.25√ó, or 1.5√ó that amount The match offer was framed both in the text of the letter and visually highlighted on the reply card included in the envelope. The control group‚Äôs reply card featured only the organization‚Äôs logo.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nIn this step, we‚Äôre comparing the ‚Äúmonths since last donation‚Äù (mrm2) between the treatment and control groups using two different methods: a manual t-test and a linear regression. The t-test helps us determine whether there‚Äôs a statistically significant difference in the average time since last donation between the two groups. Then, we use a simple linear regression (mrm2 ~ treatment) to estimate the same difference ‚Äî where the treatment variable acts as a binary indicator (1 for treatment group, 0 for control). We summarize both results in clean, readable tables to clearly report the group means, differences, and statistical significance of the treatment effect. This allows us to confirm whether the random assignment successfully balanced this pre-treatment variable, which is an important validation step in any randomized experiment.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# --- Manual T-Test ---\ntreated = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_diff = treated.mean() - control.mean()\nse_diff = np.sqrt((treated.var(ddof=1)/len(treated)) + (control.var(ddof=1)/len(control)))\nt_stat = mean_diff / se_diff\ndfree = len(treated) + len(control) - 2\np_val = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=dfree))\n\n# Create a clean results table\nt_test_table = pd.DataFrame({\n    'Group': ['Treatment', 'Control', 'Difference'],\n    'Mean (mrm2)': [treated.mean(), control.mean(), mean_diff],\n    'Std. Error': [treated.std()/np.sqrt(len(treated)), control.std()/np.sqrt(len(control)), se_diff]\n})\n\nprint(\"\\n Manual T-Test Summary:\")\nprint(t_test_table.round(4))\n\nprint(f\"\\nT-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n# --- Linear Regression ---\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\n\n# Create regression summary table\nreg_table = pd.DataFrame({\n    'Coefficient': model.params,\n    'Std. Error': model.bse,\n    't-Statistic': model.tvalues,\n    'P-Value': model.pvalues\n})\n\nprint(\"\\n Linear Regression Summary:\")\nprint(reg_table.round(4))\n\n\n Manual T-Test Summary:\n        Group  Mean (mrm2)  Std. Error\n0   Treatment      13.0118      0.0661\n1     Control      12.9981      0.0935\n2  Difference       0.0137      0.1145\n\nT-statistic: 0.1195, p-value: 0.9049\n\n Linear Regression Summary:\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept      12.9981      0.0935     138.9789   0.0000\ntreatment       0.0137      0.1145       0.1195   0.9049"
  },
  {
    "objectID": "projects/hw5/index.html#introduction",
    "href": "projects/hw5/index.html#introduction",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn their 2007 paper published in the American Economic Review, Dean Karlan and John List explored the behavioral economics of charitable giving through a large-scale natural field experiment. They sought to answer a core question in fundraising strategy: Does the way a donation appeal is framed‚Äîparticularly with the use of matching grants‚Äîsignificantly influence donor behavior? While prior research had focused heavily on tax incentives and the ‚Äúsupply side‚Äù of giving, Karlan and List shifted attention to the ‚Äúdemand side,‚Äù providing some of the first rigorous evidence on how potential donors respond to price-like mechanisms in real-world charitable campaigns.\nThis project seeks to replicate their results. ## Data\n\n\nThe experiment was conducted in collaboration with a liberal nonprofit organization in the United States that focuses on civil liberties. The researchers utilized a direct mail fundraising campaign, sending letters to over 50,000 prior donors from the organization‚Äôs database. Each recipient received a four-page fundraising letter that was identical in content, except for three randomized elements in the treatment group.\nThe individuals were divided into:\nControl Group: Received a standard donation request letter, with no mention of a matching grant. Treatment Group: Received a letter including a paragraph that announced a matching grant from a ‚Äúconcerned fellow member.‚Äù Within the treatment group, letters were further randomized across three dimensions:\nMatch Ratio: $1:$1 (every dollar donated is matched with $1) $2:$1 (every dollar is matched with $2) $3:$1 (every dollar is matched with $3) Maximum Match Amount: $25,000, $50,000, $100,000, or left unstated Suggested Donation Amounts: Based on the recipient‚Äôs previous highest donation, the reply card listed either the same amount, 1.25√ó, or 1.5√ó that amount The match offer was framed both in the text of the letter and visually highlighted on the reply card included in the envelope. The control group‚Äôs reply card featured only the organization‚Äôs logo.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nIn this step, we‚Äôre comparing the ‚Äúmonths since last donation‚Äù (mrm2) between the treatment and control groups using two different methods: a manual t-test and a linear regression. The t-test helps us determine whether there‚Äôs a statistically significant difference in the average time since last donation between the two groups. Then, we use a simple linear regression (mrm2 ~ treatment) to estimate the same difference ‚Äî where the treatment variable acts as a binary indicator (1 for treatment group, 0 for control). We summarize both results in clean, readable tables to clearly report the group means, differences, and statistical significance of the treatment effect. This allows us to confirm whether the random assignment successfully balanced this pre-treatment variable, which is an important validation step in any randomized experiment.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# --- Manual T-Test ---\ntreated = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_diff = treated.mean() - control.mean()\nse_diff = np.sqrt((treated.var(ddof=1)/len(treated)) + (control.var(ddof=1)/len(control)))\nt_stat = mean_diff / se_diff\ndfree = len(treated) + len(control) - 2\np_val = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=dfree))\n\n# Create a clean results table\nt_test_table = pd.DataFrame({\n    'Group': ['Treatment', 'Control', 'Difference'],\n    'Mean (mrm2)': [treated.mean(), control.mean(), mean_diff],\n    'Std. Error': [treated.std()/np.sqrt(len(treated)), control.std()/np.sqrt(len(control)), se_diff]\n})\n\nprint(\"\\n Manual T-Test Summary:\")\nprint(t_test_table.round(4))\n\nprint(f\"\\nT-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n# --- Linear Regression ---\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\n\n# Create regression summary table\nreg_table = pd.DataFrame({\n    'Coefficient': model.params,\n    'Std. Error': model.bse,\n    't-Statistic': model.tvalues,\n    'P-Value': model.pvalues\n})\n\nprint(\"\\n Linear Regression Summary:\")\nprint(reg_table.round(4))\n\n\n Manual T-Test Summary:\n        Group  Mean (mrm2)  Std. Error\n0   Treatment      13.0118      0.0661\n1     Control      12.9981      0.0935\n2  Difference       0.0137      0.1145\n\nT-statistic: 0.1195, p-value: 0.9049\n\n Linear Regression Summary:\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept      12.9981      0.0935     138.9789   0.0000\ntreatment       0.0137      0.1145       0.1195   0.9049"
  },
  {
    "objectID": "projects/hw5/index.html#experimental-results",
    "href": "projects/hw5/index.html#experimental-results",
    "title": "Central Limit Theorem",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nLet us visualize the donation rates for the treatment and control groups. We calculate the proportion of people who donated (gave == 1) in each group, then create a bar plot comparing these rates side by side. Each bar shows the average donation rate for its group, and we add labels above the bars to display the exact percentages. This plot gives a quick and intuitive view of how the treatment ‚Äî receiving a matching donation offer ‚Äî affected the likelihood of giving. This visualization complements our statistical analysis and helps illustrate the extensive margin effect of the treatment (i.e., whether more people chose to donate).\n\n\n\n\n\n\n\n\n\nNext step, we‚Äôre comparing donation rates between the treatment and control groups using two statistical approaches: a t-test and a simple linear regression. First, we perform an independent t-test to check if there is a statistically significant difference in the proportion of people who donated (gave) between the two groups. We summarize the group means, standard errors, and their difference in a clean table. Next, we run a bivariate linear regression (gave ~ treatment) to estimate the effect of treatment assignment on donation behavior. The treatment variable serves as a binary indicator (1 for treatment, 0 for control), and the coefficient tells us how much more (or less) likely someone is to donate if they were in the treatment group. Together, these analyses help us quantify and assess the causal impact of the matching donation offer on donation participation (i.e., the extensive margin).\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# --- Step 1: Split groups ---\ntreated = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\n# --- Step 2: T-Test ---\nt_stat, p_value = stats.ttest_ind(treated, control, equal_var=False)\n\n# --- Step 3: Create a clean summary table for T-Test ---\nt_test_table = pd.DataFrame({\n    'Group': ['Treatment', 'Control'],\n    'Mean (gave)': [treated.mean(), control.mean()],\n    'Standard Error': [\n        treated.std(ddof=1) / np.sqrt(len(treated)),\n        control.std(ddof=1) / np.sqrt(len(control))\n    ]\n})\n\n# Add difference row with np.nan for missing Std. Error (to avoid FutureWarning)\nt_test_table.loc[2] = ['Difference', treated.mean() - control.mean(), np.nan]\n\nprint(\"üìä T-Test Summary:\\n\")\nprint(t_test_table.round(4))\n\nprint(f\"\\nt-statistic: {t_stat:.4f}\")\nprint(f\"p-value:     {p_value:.4f}\")\n\n# --- Step 4: Linear Regression ---\nmodel = smf.ols('gave ~ treatment', data=df).fit()\n\n# --- Step 5: Format regression summary into a table ---\nreg_table = pd.DataFrame({\n    'Coefficient': model.params,\n    'Std. Error': model.bse,\n    't-Statistic': model.tvalues,\n    'P-Value': model.pvalues\n}).round(4)\n\nprint(\"\\nüìâ Linear Regression Summary:\\n\")\nprint(reg_table)\n\nüìä T-Test Summary:\n\n        Group  Mean (gave)  Standard Error\n0   Treatment       0.0220          0.0008\n1     Control       0.0179          0.0010\n2  Difference       0.0042             NaN\n\nt-statistic: 3.2095\np-value:     0.0013\n\nüìâ Linear Regression Summary:\n\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept       0.0179      0.0011      16.2246   0.0000\ntreatment       0.0042      0.0013       3.1014   0.0019\n\n\nWhat This Means About Human Behavior We‚Äôve learned that people are more likely to give when they know their donation will be matched. Even though the increase might seem small numerically, the effect is meaningful: simply mentioning a matching gift nudges more people into taking action.\nThis tells us that:\nSocial cues matter. When people know others are also giving (like a ‚Äúconcerned member‚Äù offering a match), it makes them feel part of something. Framing matters. The idea that their donation will ‚Äúgo further‚Äù encourages behavior change. Behavior is not purely rational ‚Äî a simple sentence in a letter changes what people do with their money. This is why Table 2A (Panel A) in the paper is so important ‚Äî it quantifies how a subtle psychological nudge leads to real-world donations.\nNow let us estimate a probit regression model to analyze the effect of the treatment on the likelihood of making a donation. The dependent variable is gave (a binary indicator for whether the individual donated), and the independent variable is treatment (1 if the person received the matching offer, 0 otherwise). Using a probit model allows us to model the probability of donating as a nonlinear function of the treatment assignment, assuming a standard normal distribution of the error term. This method is appropriate for binary outcome variables and provides a more statistically rigorous approach than a linear probability model. The results will help us assess whether the treatment significantly increases the probability of giving, and by how much (on a latent scale).\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Run the probit model: gave ~ treatment\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\n\n# Print summary\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 10 Aug 2025   Pseudo R-squ.:               0.0009783\nTime:                        09:47:32   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions‚Ä¶\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nfrom scipy import stats\n\n# Filter treatment group only\nmatch_group = df[df['treatment'] == 1]\n\n# Define donation status for each ratio group\ngave_ratio_1 = match_group[match_group['ratio'] == 1]['gave']\ngave_ratio_2 = match_group[match_group['ratio'] == 2]['gave']\ngave_ratio_3 = match_group[match_group['ratio'] == 3]['gave']\n\n# --- 1:1 vs 2:1 ---\ntstat_1v2, pval_1v2 = stats.ttest_ind(gave_ratio_1, gave_ratio_2, equal_var=False)\nprint(\"T-test: 1:1 vs 2:1\")\nprint(f\"t-statistic = {tstat_1v2:.4f}, p-value = {pval_1v2:.4f}\")\n\n# --- 1:1 vs 3:1 ---\ntstat_1v3, pval_1v3 = stats.ttest_ind(gave_ratio_1, gave_ratio_3, equal_var=False)\nprint(\"\\nT-test: 1:1 vs 3:1\")\nprint(f\"t-statistic = {tstat_1v3:.4f}, p-value = {pval_1v3:.4f}\")\n\n# --- 2:1 vs 3:1 ---\ntstat_2v3, pval_2v3 = stats.ttest_ind(gave_ratio_2, gave_ratio_3, equal_var=False)\nprint(\"\\nT-test: 2:1 vs 3:1\")\nprint(f\"t-statistic = {tstat_2v3:.4f}, p-value = {pval_2v3:.4f}\")\n\nT-test: 1:1 vs 2:1\nt-statistic = -0.9650, p-value = 0.3345\n\nT-test: 1:1 vs 3:1\nt-statistic = -1.0150, p-value = 0.3101\n\nT-test: 2:1 vs 3:1\nt-statistic = -0.0501, p-value = 0.9600\n\n\n‚ÄúWhile the match treatments relative to a control group increase the probability of donating, larger match ratios‚Äî$3:$1 and $2:$1‚Äîrelative to a smaller match ratio ($1:$1) have no additional impact.‚Äù‚Äìfrom page 8\nYes ‚Äî the results support the ‚Äúfigures suggest‚Äù comment made by the authors on page 8 of the Karlan & List (2007) paper. All p-values are well above 0.05, which means that none of the differences between match ratios are statistically significant. In other words, there‚Äôs no evidence that higher match ratios (like 2:1 or 3:1) increased the likelihood of giving compared to a 1:1 match.\nNext, we‚Äôre analyzing whether the size of the match ratio (1:1, 2:1, or 3:1) affects the likelihood that someone donates ‚Äî but only among individuals who were in the treatment group (i.e., who received a match offer). We approach this in two ways: 1. Dummy variable regression: We create binary variables for each match ratio and run a regression using gave as the outcome and ratio2 and ratio3 as predictors, with ratio1 (1:1 match) serving as the baseline. This lets us interpret the coefficients as changes in donation probability relative to the 1:1 group. 2. Categorical variable regression: We use C(ratio) to treat the ratio variable as a categorical factor. This achieves the same comparison as above but allows statsmodels to handle dummy coding internally. Both models help us determine whether offering a more generous match (like 2:1 or 3:1) significantly increases the probability of donation compared to a standard 1:1 match. The resulting tables summarize the estimated effects, their statistical significance, and allow us to assess whether larger match ratios are more effective. -\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# 1. Filter for treatment group only (those who received a match offer)\nmatch_df = df[df['treatment'] == 1].copy()\n\n# 2. Create dummy variables for match ratio\nmatch_df['ratio1'] = (match_df['ratio'] == 1).astype(int)\nmatch_df['ratio2'] = (match_df['ratio'] == 2).astype(int)\nmatch_df['ratio3'] = (match_df['ratio'] == 3).astype(int)\n\n# 3. Regression using ratio2 and ratio3 (ratio1 is baseline)\nmodel_dummies = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\n\n# 4. Regression using ratio as categorical variable\nmodel_cat = smf.ols('gave ~ C(ratio)', data=match_df).fit()\n\n# Format model_dummies summary\nreg_dummies_table = pd.DataFrame({\n    'Coefficient': model_dummies.params,\n    'Std. Error': model_dummies.bse,\n    't-Statistic': model_dummies.tvalues,\n    'P-Value': model_dummies.pvalues\n}).round(4)\n\n# Format model_cat summary\nreg_cat_table = pd.DataFrame({\n    'Coefficient': model_cat.params,\n    'Std. Error': model_cat.bse,\n    't-Statistic': model_cat.tvalues,\n    'P-Value': model_cat.pvalues\n}).round(4)\n\n# Display tables\nprint(\"üìâ Regression using dummy variables (baseline is ratio1):\\n\")\nprint(reg_dummies_table)\n\nprint(\"\\nüìä Regression using C(ratio) as a categorical variable:\\n\")\nprint(reg_cat_table)\n\nüìâ Regression using dummy variables (baseline is ratio1):\n\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept       0.0207      0.0014      14.9122   0.0000\nratio2          0.0019      0.0020       0.9576   0.3383\nratio3          0.0020      0.0020       1.0083   0.3133\n\nüìä Regression using C(ratio) as a categorical variable:\n\n                Coefficient    Std. Error  t-Statistic  P-Value\nIntercept      1.229429e+09  1.118256e+10       0.1099   0.9125\nC(ratio)[T.1] -1.229429e+09  1.118256e+10      -0.1099   0.9125\nC(ratio)[T.2] -1.229429e+09  1.118256e+10      -0.1099   0.9125\nC(ratio)[T.3] -1.229429e+09  1.118256e+10      -0.1099   0.9125\n\n\nInterpretation: - People in the 1:1 match group donated at a rate of about 2.07%. - Offering a more generous match ‚Äî 2:1 or 3:1 ‚Äî slightly increased the donation rate by about 0.2 percentage points, but this difference was not statistically significant. - The p-values for both ratio2 and ratio3 are well above 0.05, meaning we cannot conclude that these match levels had a meaningful effect on donor behavior.\n\nNext step: We‚Äôll evaluate how the size of the match ratio affects donation rates by comparing the response rate differences between the 1:1 and 2:1 match groups, and between the 2:1 and 3:1 groups. We‚Äôll do this in two ways: first, by calculating the differences directly from the observed data, and second, by comparing the coefficients from our earlier regression models. This will help us assess whether offering more generous match ratios leads to meaningfully higher donation rates. We‚Äôll then draw conclusions about the practical effectiveness of increasing the match size.\n\n# Step 1: Filter to treatment group\nmatch_df = df[df['treatment'] == 1]\n\n# Step 2: Calculate response rates directly from data\nrate_1 = match_df[match_df['ratio'] == 1]['gave'].mean()\nrate_2 = match_df[match_df['ratio'] == 2]['gave'].mean()\nrate_3 = match_df[match_df['ratio'] == 3]['gave'].mean()\n\ndiff_1v2 = rate_2 - rate_1\ndiff_2v3 = rate_3 - rate_2\n\n# Step 3: Use regression coefficients (from earlier model)\n# These should match your actual model output ‚Äî adjust if needed\nrate_1_pred = 0.0207        # Intercept\ncoef_ratio2 = 0.0019\ncoef_ratio3 = 0.0020\n\nrate_2_pred = rate_1_pred + coef_ratio2\nrate_3_pred = rate_1_pred + coef_ratio3\n\ndiff_1v2_pred = rate_2_pred - rate_1_pred\ndiff_2v3_pred = rate_3_pred - rate_2_pred\n\n# Step 4: Display results\nprint(\"üî¢ Direct from data:\")\nprint(f\"Response rate (1:1): {rate_1:.4f}\")\nprint(f\"Response rate (2:1): {rate_2:.4f}\")\nprint(f\"Response rate (3:1): {rate_3:.4f}\")\nprint(f\"1:1 vs 2:1 match difference: {diff_1v2:.4f}\")\nprint(f\"2:1 vs 3:1 match difference: {diff_2v3:.4f}\")\n\nprint(\"\\nüìä From regression coefficients:\")\nprint(f\"Predicted rate (1:1): {rate_1_pred:.4f}\")\nprint(f\"Predicted rate (2:1): {rate_2_pred:.4f}\")\nprint(f\"Predicted rate (3:1): {rate_3_pred:.4f}\")\nprint(f\"1:1 vs 2:1 match difference (predicted): {diff_1v2_pred:.4f}\")\nprint(f\"2:1 vs 3:1 match difference (predicted): {diff_2v3_pred:.4f}\")\n\nüî¢ Direct from data:\nResponse rate (1:1): 0.0207\nResponse rate (2:1): 0.0226\nResponse rate (3:1): 0.0227\n1:1 vs 2:1 match difference: 0.0019\n2:1 vs 3:1 match difference: 0.0001\n\nüìä From regression coefficients:\nPredicted rate (1:1): 0.0207\nPredicted rate (2:1): 0.0226\nPredicted rate (3:1): 0.0227\n1:1 vs 2:1 match difference (predicted): 0.0019\n2:1 vs 3:1 match difference (predicted): 0.0001\n\n\nFrom both your raw data and regression predictions, here‚Äôs what we can conclude:\n-Moving from a 1:1 to a 2:1 match increases the donation rate by just 0.19 percentage points. -Moving from a 2:1 to a 3:1 match increases the rate by only 0.01 percentage points. -These changes are extremely small and, as your earlier t-tests and regression showed, not statistically significant.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n*First step: Perform a t-test or simple linear regression to analyze whether being in the treatment group had an effect on the amount donated**. This helps us understand whether the matching offer influenced not just whether people gave, but how much they gave. We‚Äôll interpret the results to assess the impact of the treatment on donation size.\n\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Step 1: Split into treatment and control groups\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['treatment'] == 0]['amount']\n\n# Step 2: T-test\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Create clean summary table\nt_test_table = pd.DataFrame({\n    'Group': ['Treatment', 'Control', 'Difference'],\n    'Mean (amount)': [\n        amount_treat.mean(),\n        amount_control.mean(),\n        amount_treat.mean() - amount_control.mean()\n    ],\n    'Std. Error': [\n        amount_treat.std(ddof=1) / len(amount_treat)**0.5,\n        amount_control.std(ddof=1) / len(amount_control)**0.5,\n        None\n    ]\n}).round(4)\n\nprint(\"üî¢ T-Test Summary Table:\\n\")\nprint(t_test_table)\n\nprint(f\"\\nt-statistic: {t_stat:.4f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Step 3: Linear Regression\nmodel = smf.ols('amount ~ treatment', data=df).fit()\n\n# Create regression result table\nreg_table = pd.DataFrame({\n    'Coefficient': model.params,\n    'Std. Error': model.bse,\n    't-Statistic': model.tvalues,\n    'P-Value': model.pvalues\n}).round(4)\n\nprint(\"\\nüìä Linear Regression Summary Table:\\n\")\nprint(reg_table)\n\nüî¢ T-Test Summary Table:\n\n        Group  Mean (amount)  Std. Error\n0   Treatment         0.9669      0.0490\n1     Control         0.8133      0.0633\n2  Difference         0.1536         NaN\n\nt-statistic: 1.9183\np-value:     0.0551\n\nüìä Linear Regression Summary Table:\n\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept       0.8133      0.0674      12.0630   0.0000\ntreatment       0.1536      0.0826       1.8605   0.0628\n\n\n-This analysis shows that offering a match might increase not only the likelihood of giving but also the amount given, though the evidence is not quite strong enough to be statistically conclusive at the standard 95% confidence level.\n-So far, the match offer seems to mainly help on the extensive margin ‚Äî getting more people to donate. Its effect on the intensive margin ‚Äî how much people give ‚Äî appears small and uncertain.\n\nNext step: Focus only on individuals who actually made a donation (gave == 1) and re-run the previous analysis using this subset. This allows us to examine how much people donate conditional on giving. We‚Äôll interpret the regression results to understand whether the treatment influenced the donation amount among donors, and consider whether the treatment effect in this case can be interpreted causally\n\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Step 1: Filter to donors only\ndonors_df = df[df['gave'] == 1]\n\n# Step 2: Split groups\namount_treat = donors_df[donors_df['treatment'] == 1]['amount']\namount_control = donors_df[donors_df['treatment'] == 0]['amount']\n\n# Step 3: T-test\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Step 4: T-test Summary Table\nt_test_table = pd.DataFrame({\n    'Group': ['Treatment', 'Control', 'Difference'],\n    'Mean (amount)': [\n        amount_treat.mean(),\n        amount_control.mean(),\n        amount_treat.mean() - amount_control.mean()\n    ],\n    'Std. Error': [\n        amount_treat.std(ddof=1) / len(amount_treat)**0.5,\n        amount_control.std(ddof=1) / len(amount_control)**0.5,\n        None\n    ]\n}).round(4)\n\nprint(\"üî¢ T-Test Summary (Among Donors Only):\\n\")\nprint(t_test_table)\nprint(f\"\\nt-statistic: {t_stat:.4f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Step 5: Regression (OLS)\nmodel = smf.ols('amount ~ treatment', data=donors_df).fit()\n\n# Step 6: Regression Summary Table\nreg_table = pd.DataFrame({\n    'Coefficient': model.params,\n    'Std. Error': model.bse,\n    't-Statistic': model.tvalues,\n    'P-Value': model.pvalues\n}).round(4)\n\nprint(\"\\nüìä Linear Regression Summary (Among Donors Only):\\n\")\nprint(reg_table)\n\nüî¢ T-Test Summary (Among Donors Only):\n\n        Group  Mean (amount)  Std. Error\n0   Treatment      43.871899      1.5487\n1     Control      45.540298      2.3971\n2  Difference      -1.668400         NaN\n\nt-statistic: -0.5846\np-value:     0.5590\n\nüìä Linear Regression Summary (Among Donors Only):\n\n           Coefficient  Std. Error  t-Statistic  P-Value\nIntercept      45.5403      2.4234      18.7921   0.0000\ntreatment      -1.6684      2.8724      -0.5808   0.5615\n\n\nThe coefficient on treatment is ‚Äì1.67, which means that, among those who donated, people in the treatment group gave $1.67 less on average than those in the control group. However, the p-value is 0.561, indicating that this difference is not statistically significant. In short, we find no evidence that the treatment affected the amount donated among those who gave.\nWe learned that the matching grant offer did not change how much people gave, once they decided to donate. The main impact of the treatment was likely on getting people to donate in the first place (the extensive margin), not on how much they donated (the intensive margin). So the match was effective at increasing participation, but not effective at increasing donation size among participants.\nDoes the treatment coefficient have a causal interpretation? Answer: No, the treatment coefficient in this regression does not have a valid causal interpretation. Because the regression is conditional on donating (i.e., only includes people for whom gave == 1). But treatment itself influences who ends up in this group ‚Äî meaning we‚Äôre analyzing a selected subgroup that may differ systematically between treatment and control.\nThis introduces selection bias, so the regression tells us about differences among donors, but not about the causal effect of treatment on donation size.\n\nThen: We‚Äôll create two histograms ‚Äî one for the treatment group and one for the control group ‚Äî showing the distribution of donation amounts only among individuals who made a donation. We‚Äôll also add a red vertical line to each plot to mark the average donation amount, allowing us to visually compare the giving behavior between the two groups."
  },
  {
    "objectID": "projects/hw5/index.html#simulation-experiment",
    "href": "projects/hw5/index.html#simulation-experiment",
    "title": "Central Limit Theorem",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nFirst: We‚Äôll simulate 10,000 individual outcomes from both the treatment and control groups using their actual donation probabilities. For each simulated pair, we‚Äôll compute the difference in giving behavior (1 or 0). By calculating the cumulative average of these 10,000 differences, we‚Äôll observe how the estimated treatment effect evolves with increasing sample size. The plot will start off noisy, but it should stabilize around the true effect (~0.004) as the number of simulations grows. We‚Äôll use this to visually demonstrate the Law of Large Numbers and explain the result to the reader.\n\n\n\n\n\n\n\n\n\n-At first (left side of the plot), the estimate is highly volatile ‚Äî bouncing around because it‚Äôs based on only a few observations. -As the number of simulations increases (moving right), the average stabilizes and converges to the true treatment effect. -This is a practical demonstration of the Law of Large Numbers: the more data we gather, the more reliable our estimate becomes.\nAs the number of simulations increases (moving right), the average stabilizes and converges to the true treatment effect. This is a practical demonstration of the Law of Large Numbers: the more data we gather, the more reliable our estimate becomes.\n\n\nCentral Limit Theorem\n\nNow We‚Äôll simulate sampling from the treatment and control groups at four different sample sizes: 50, 200, 500, and 1000. For each sample size, we‚Äôll draw 50 (or more) observations from each group, calculate the average difference in donation rates, and repeat this process 1,000 times to generate a distribution of sample differences. Then, we‚Äôll plot a histogram of those 1,000 differences for each sample size. This series of histograms will help demonstrate how, as sample size increases, the distribution of sample averages becomes narrower and more centered around the true treatment effect ‚Äî a visual illustration of the Central Limit Theorem.\n\n\n\n\n\n\n\n\n\n-Sample Size = 50: The distribution is wide and erratic. The sample mean differences vary a lot ‚Äî some simulations overestimate the effect, others underestimate it. The shape is not very normal.\n-Sample Size = 200: The distribution begins to tighten. It‚Äôs more centered around the true effect, though still somewhat spread out.\n-Sample Size = 500: The distribution is clearly bell-shaped, centered around the estimated effect, with less variation.\n-Sample Size = 1000: The distribution is even tighter and smoother. Most estimates fall within a narrow range around the true effect of ~0.0045.\nOverall, this progression of histograms visually demonstrates the Central Limit Theorem in action. As the sample size increases, the variability of the sampling distribution decreases, and the distribution becomes more symmetric and concentrated around the true treatment effect. This shows that with larger samples, our estimates become more reliable and precise ‚Äî even when the underlying effect is small. It‚Äôs a powerful reminder that sample size plays a critical role in detecting and confidently estimating treatment effects in experimental data."
  },
  {
    "objectID": "projects/hw1/index1.html",
    "href": "projects/hw1/index1.html",
    "title": "TZ Gaming: Optimal Targeting of Mobile Ads",
    "section": "",
    "text": "As a developer of games for mobile devices, TZ gaming has achieved strong growth of its customer base. A prominent source of new customers has come from ads displayed through the Vneta ad-network. A mobile-ad network is a technology platform that serves as a broker between (1) app developers (or publishers) looking to sell ad space and (2) a group of advertisers.\nApp developers sell ‚Äúimpressions‚Äù, i.e., a space where an ad can be shown, through the Vneta network to companies such as TZ gaming looking to advertise to app users. Vneta acts as a broker for 50-60 millions impressions/ads per day. TZ gaming uses ads to appeal to prospective customers for their games. They generally use short (15 sec) video ads that help to emphasize the dynamic nature of the games. In the past, TZ has been able to, approximately, break-even on ad-spend with Vneta when calculating the benefits that can be directly attributed to ad click-through. Many senior executives at TZ believe that there are additional, longer-term, benefits from these ads such as brand awareness, etc. that are harder to quantify.\nCurrently, TZ has access to very limited data from Vneta. Matt Huateng, the CEO of TZ gaming, is intrigued by the potential for data science to enhance the efficiency of targeted advertising on mobile devices. Specifically, two options are under consideration: (1) Buy access to additional data from Vneta and use TZ‚Äôs analytics team to build targeting models or (2) Subscribe to Vneta‚Äôs analytics consultancy service, which provides impression-level click-through rate predictions based on Vneta‚Äôs proprietary data and algorithms.\nVneta has shared behavioral information linked to 115,488 recent impressions used to show TZ ads and has also provided a set or predictions based on their own (proprietary) algorithm. Matt is unsure if the consulting services offered by Vneta will be worth the money for future ad campaigns and has asked you to do some initial analyses on the provided data and compare the generated predictions to Vneta‚Äôs recommendations. The following targeting options will be evaluated to determine the best path forward."
  },
  {
    "objectID": "projects/hw1/data/tz_gaming_description.html",
    "href": "projects/hw1/data/tz_gaming_description.html",
    "title": "Mario",
    "section": "",
    "text": "TZ Gaming: Optimal Targeting of Mobile Ads\nEach row in the tz_gaming dataset represents an impression. For each row (impression), we have data on 21 variables. All explanatory variables are created by Vneta based on one month tracking history of users, apps, and ads. The available variables are described in below.\n\ntraining ‚Äì Dummy variable that splits the dataset into a training (‚Äútrain‚Äù) and a test (‚Äútest‚Äù) set\ninum ‚Äì Impression number\nclick ‚Äì Click indicator for the TZ ad served in the impression. Equals ‚Äúyes‚Äù if the ad was clicked and ‚Äúno‚Äù otherwise\ntime ‚Äì The hour of the day in which the impression occurred (1-24). For example, ‚Äú2‚Äù indicates the impression occurred between 1 am and 2 am\ntime_fct ‚Äì Same as time but the is coded as categorical\napp ‚Äì The app in which the impression was shown. Ranges from 1 to 49\nmobile_os ‚Äì Customer‚Äôs mobile OS\nimpup ‚Äì Number of past impressions the user has seen in the app\nclup ‚Äì Number of past impressions the user has clicked on in the app\nctrup ‚Äì Past CTR (Click-Through Rate) (x 100) for the user in the app\nimpua ‚Äì Number of past impressions of the TZ ad that the user has seen across all apps\nclua ‚Äì Number of past impressions of the TZ ad that the user has clicked on across all apps\nctrua ‚Äì Past CTR (x 100) of the TZ ad by the user across all apps\nimput ‚Äì Number of past impressions the user has seen within in the hour\nclut ‚Äì Number of past impressions the user has clicked on in the hour\nctrut ‚Äì Past CTR (x 100) of the user in the hour\nimppat ‚Äì Number of past impressions that showed the TZ ad in the app in the hour\nclpat ‚Äì Number of past clicks the TZ ad has received in the app in the hour\nctrpat ‚Äì Past CTR (x 100) of the TZ ad in the app in the hour\nrnd ‚Äì Simulated data from a normal distribution with mean 0 and a standard deviation of 1\npred_vneta ‚Äì Predicted probability of click per impressions generated by Vneta‚Äôs proprietary machine learning algorithm\nid ‚Äì Anonymized user ID\n\nNote that there is a clear relationship between the impressions, clicks, and ctr variables within a strata. Specifically:\n\nctrup = clup/impup\nctru = clu/impu\nctrut = clut/imput\nctrpat = clpat/impat\n\nThe last three letters of a feature indicate the sources of variation in a variable:\n\nu ‚Äî denotes user\nt ‚Äî denotes time\np ‚Äî denotes app\na ‚Äî denotes ad"
  },
  {
    "objectID": "projects/hw1/index1.html#multicollinearity-and-omitted-variable-bias",
    "href": "projects/hw1/index1.html#multicollinearity-and-omitted-variable-bias",
    "title": "TZ Gaming: Optimal Targeting of Mobile Ads",
    "section": "Multicollinearity and Omitted Variable Bias",
    "text": "Multicollinearity and Omitted Variable Bias\n\nclf_mc1 = rsm.model.logistic(\n    data= {\"tz_train\":tz_train},\n    rvar= \"click\",\n    lev= \"yes\",\n    evar= [\"imppat\", \"clpat\", \"ctrpat\"]\n)\nclf_mc1.summary()\n\nLogistic regression (GLM)\nData                 : tz_train\nResponse variable    : click\nLevel                : yes\nExplanatory variables: imppat, clpat, ctrpat\nNull hyp.: There is no effect of x on click\nAlt. hyp.: There is an effect of x on click\n\n              OR     OR%  coefficient  std.error  z.value p.value     \nIntercept  0.004  -99.6%       -5.419      0.073  -74.156  &lt; .001  ***\nimppat     1.000   -0.0%       -0.000      0.000   -4.802  &lt; .001  ***\nclpat      1.002    0.2%        0.002      0.000    5.713  &lt; .001  ***\nctrpat     1.615   61.5%        0.479      0.034   13.933  &lt; .001  ***\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.035\nPseudo R-squared (McFadden adjusted): 0.035\nArea under the RO Curve (AUC): 0.676\nLog-likelihood: -4273.088, AIC: 8554.176, BIC: 8591.695\nChi-squared: 314.248, df(3), p.value &lt; 0.001 \nNr obs: 87,535\n\n\n\nChi-Square Citical Value\n\nfrom scipy.stats import chi2\ndf=3\nalpha_0_05 = chi2.ppf(1-0.05,df)\nalpha_0_01= chi2.ppf(1-0.01,df)\nprint(alpha_0_05,alpha_0_01)\n\n7.8147279032511765 11.34486673014437\n\n\n\n\nPlot\n\nclf_mc1.plot(\n    plots=\"pred\", incl=[\"imppat\", \"clpat\", \"ctrpat\"]\n)\n\n\n\n\n\n\n\n\n\n\nPermutation Importance\n\nclf_mc1.plot(plots=\"vimp\")\n\n\n\n\n\n\n\n\n\n\nInterpretation\nFor Odd Ratio, when you increase the ctrpat (click throgh put rate) by one unit, the odds of the outcome which is clicking increases by a factor of 1.615 or 61.5% while other variables remain constant.Meanwhile, a unit increase only by a factor of 1.002 for clpt (past clicks in a specific hour) and no effect on odd of clicking for the feature imppat (the number of past impression that showed a TZ ad)\nUsing the coefficient and the plot, an increase in the number of past impression that showed a TZ ad(immpat) has a insignificant negative correlation(-0.00) with odds of clicking. This shows in both regression and plot. Then, there is a slight positive coeffecient(0.00) between clpat and the odds of clicking. Finally, we have considerable positive correlation(0.48) between ctrpat and the odds of clicking. Both of these shows in the plot as well.\nall p_values are less than 0.05 implies on the significant effect of imppat, ctrpat and clpat on the odds of clicking. In fact, they all have three asterisk which amplifies the level of statistical significance of these three variables.\n\nCorrelation\n\ntz_train[['imppat', 'clpat', 'ctrpat']].corr()\n\n\n\n\n\n\n\n\nimppat\nclpat\nctrpat\n\n\n\n\nimppat\n1.000000\n0.971579\n0.344099\n\n\nclpat\n0.971579\n1.000000\n0.460035\n\n\nctrpat\n0.344099\n0.460035\n1.000000\n\n\n\n\n\n\n\n\nclf_mc2 = rsm.model.logistic(\n    data= {\"tz_train\": tz_train},\n    rvar = \"click\",\n    lev= \"yes\",\n    evar =[\"imppat\", \"ctrpat\"],\n )\nclf_mc2.summary()\n\nLogistic regression (GLM)\nData                 : tz_train\nResponse variable    : click\nLevel                : yes\nExplanatory variables: imppat, ctrpat\nNull hyp.: There is no effect of x on click\nAlt. hyp.: There is an effect of x on click\n\n              OR     OR%  coefficient  std.error  z.value p.value     \nIntercept  0.004  -99.6%       -5.529      0.068  -80.814  &lt; .001  ***\nimppat     1.000    0.0%        0.000      0.000    5.460  &lt; .001  ***\nctrpat     1.733   73.3%        0.550      0.030   18.422  &lt; .001  ***\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.031\nPseudo R-squared (McFadden adjusted): 0.031\nArea under the RO Curve (AUC): 0.674\nLog-likelihood: -4290.903, AIC: 8587.805, BIC: 8615.945\nChi-squared: 278.619, df(2), p.value &lt; 0.001 \nNr obs: 87,535\n\n\n\nclf_mc2.plot(\"pred\")\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe coefficient of ctrpat increased from 0.48 to 0.55, and the odds ratio rose from 61.5% to 73.3%. This suggests that removing clpat revealed the true impact of the remaining explanatory variables. While Pseudo R-Squared and AUC were not significantly affected, the predicted plot now shows a clearer relationship between the odds of clicking and the variables imppat and ctrpat. Since imppat and clpat have a high correlation of 0.97, multicollinearity exists, which can make coefficient estimates unstable and affect model interpretation. By omitting one of these highly correlated variables, we mitigate this issue, leading to a more interpretable model. This is evident in the plot, where the relationship between ctrpat, imppat, and the odds of clicking is now more clearly observed.\n\n\nDifference After Adding More Features\n\nclf_mc3 = rsm.model.logistic (\n    data = {\"tz_train\": tz_train},\n    rvar= \"click\",\n    lev= \"yes\",\n    evar=[\"time_fct\", \"app\", \"imppat\", \"clpat\", \"ctrpat\"]\n)\n\nclf_mc3.summary()\n\nLogistic regression (GLM)\nData                 : tz_train\nResponse variable    : click\nLevel                : yes\nExplanatory variables: time_fct, app, imppat, clpat, ctrpat\nNull hyp.: There is no effect of x on click\nAlt. hyp.: There is an effect of x on click\n\n                 OR      OR%  coefficient   std.error  z.value p.value     \nIntercept     0.012   -98.8%       -4.424       0.284  -15.575  &lt; .001  ***\ntime_fct[2]   0.588   -41.2%       -0.532       0.323   -1.644     0.1     \ntime_fct[3]   0.693   -30.7%       -0.367       0.461   -0.795   0.426     \ntime_fct[4]   0.000  -100.0%      -23.834   43904.903   -0.001     1.0     \ntime_fct[5]   0.000  -100.0%      -23.860   57249.164   -0.000     1.0     \ntime_fct[6]   0.359   -64.1%       -1.025       1.027   -0.999   0.318     \ntime_fct[7]   1.220    22.0%        0.199       0.438    0.453    0.65     \ntime_fct[8]   1.135    13.5%        0.127       0.309    0.411   0.681     \ntime_fct[9]   1.063     6.3%        0.061       0.299    0.203   0.839     \ntime_fct[10]  0.843   -15.7%       -0.170       0.303   -0.561   0.575     \ntime_fct[11]  0.637   -36.3%       -0.451       0.288   -1.565   0.118     \ntime_fct[12]  0.834   -16.6%       -0.181       0.292   -0.620   0.535     \ntime_fct[13]  0.535   -46.5%       -0.626       0.306   -2.047   0.041    *\ntime_fct[14]  0.982    -1.8%       -0.018       0.257   -0.069   0.945     \ntime_fct[15]  0.840   -16.0%       -0.174       0.272   -0.639   0.523     \ntime_fct[16]  0.874   -12.6%       -0.135       0.279   -0.483   0.629     \ntime_fct[17]  0.864   -13.6%       -0.146       0.296   -0.493   0.622     \ntime_fct[18]  0.942    -5.8%       -0.060       0.286   -0.208   0.835     \ntime_fct[19]  1.178    17.8%        0.164       0.251    0.651   0.515     \ntime_fct[20]  1.188    18.8%        0.172       0.247    0.698   0.485     \ntime_fct[21]  0.782   -21.8%       -0.245       0.261   -0.942   0.346     \ntime_fct[22]  0.933    -6.7%       -0.069       0.259   -0.267   0.789     \ntime_fct[23]  0.993    -0.7%       -0.007       0.268   -0.025    0.98     \ntime_fct[24]  1.134    13.4%        0.125       0.258    0.486   0.627     \napp[app2]     0.136   -86.4%       -1.997       0.351   -5.681  &lt; .001  ***\napp[app3]     0.187   -81.3%       -1.675       1.016   -1.649   0.099    .\napp[app4]     0.487   -51.3%       -0.719       0.359   -2.004   0.045    *\napp[app5]     0.413   -58.7%       -0.883       1.011   -0.874   0.382     \napp[app6]     1.007     0.7%        0.007       0.216    0.034   0.973     \napp[app7]     0.708   -29.2%       -0.346       1.019   -0.340   0.734     \napp[app8]     0.000  -100.0%      -24.110   72717.593   -0.000     1.0     \napp[app9]     0.935    -6.5%       -0.067       0.271   -0.248   0.804     \napp[app10]    0.000  -100.0%      -24.021   75559.599   -0.000     1.0     \napp[app11]    1.365    36.5%        0.311       0.752    0.414   0.679     \napp[app12]    0.575   -42.5%       -0.553       0.274   -2.022   0.043    *\napp[app13]    2.790   179.0%        1.026       0.504    2.037   0.042    *\napp[app14]    0.246   -75.4%       -1.402       0.479   -2.927   0.003   **\napp[app15]    0.773   -22.7%       -0.258       0.723   -0.357   0.721     \napp[app16]    0.381   -61.9%       -0.965       0.733   -1.316   0.188     \napp[app17]    1.502    50.2%        0.407       0.727    0.560   0.576     \napp[app18]    0.108   -89.2%       -2.226       1.016   -2.190   0.029    *\napp[app19]    0.000  -100.0%      -24.046   93787.196   -0.000     1.0     \napp[app20]    0.000  -100.0%      -24.145   73366.877   -0.000     1.0     \napp[app21]    0.475   -52.5%       -0.744       0.416   -1.789   0.074    .\napp[app22]    0.517   -48.3%       -0.660       1.019   -0.648   0.517     \napp[app23]    3.549   254.9%        1.267       0.419    3.023   0.003   **\napp[app24]    2.411   141.1%        0.880       0.363    2.421   0.015    *\napp[app25]    1.571    57.1%        0.452       0.612    0.738    0.46     \napp[app26]    0.000  -100.0%      -24.073   63336.276   -0.000     1.0     \napp[app27]    0.292   -70.8%       -1.230       0.731   -1.683   0.092    .\napp[app28]    0.318   -68.2%       -1.144       0.603   -1.897   0.058    .\napp[app29]    2.584   158.4%        0.949       0.379    2.502   0.012    *\napp[app30]    0.000  -100.0%      -24.080   87078.077   -0.000     1.0     \napp[app31]    0.164   -83.6%       -1.808       0.735   -2.460   0.014    *\napp[app32]    0.000  -100.0%      -24.044   56046.291   -0.000     1.0     \napp[app33]    0.524   -47.6%       -0.647       0.376   -1.720   0.086    .\napp[app34]    0.000  -100.0%      -24.014   55897.140   -0.000     1.0     \napp[app35]    0.357   -64.3%       -1.030       1.019   -1.011   0.312     \napp[app36]    0.000  -100.0%      -24.001   78847.008   -0.000     1.0     \napp[app37]    0.741   -25.9%       -0.300       0.732   -0.410   0.682     \napp[app38]    0.000  -100.0%      -24.036   55273.917   -0.000     1.0     \napp[app39]    2.349   134.9%        0.854       0.449    1.904   0.057    .\napp[app40]    1.625    62.5%        0.486       0.530    0.915    0.36     \napp[app41]    0.816   -18.4%       -0.204       0.725   -0.281   0.779     \napp[app42]    2.909   190.9%        1.068       0.452    2.363   0.018    *\napp[app43]    0.000  -100.0%      -24.109   75901.133   -0.000     1.0     \napp[app44]    0.000  -100.0%      -24.047   46569.636   -0.001     1.0     \napp[app45]    1.920    92.0%        0.652       0.604    1.079   0.281     \napp[app46]    0.466   -53.4%       -0.763       1.020   -0.748   0.455     \napp[app47]    0.406   -59.4%       -0.902       1.012   -0.891   0.373     \napp[app48]    0.000  -100.0%      -24.045  106095.563   -0.000     1.0     \napp[app49]    0.259   -74.1%       -1.349       1.019   -1.324   0.185     \nimppat        1.000    -0.0%       -0.000       0.000   -1.131   0.258     \nclpat         1.001     0.1%        0.001       0.001    1.135   0.256     \nctrpat        1.077     7.7%        0.075       0.117    0.637   0.524     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPseudo R-squared (McFadden): 0.056\nPseudo R-squared (McFadden adjusted): 0.04\nArea under the RO Curve (AUC): 0.704\nLog-likelihood: -4180.646, AIC: 8511.292, BIC: 9214.776\nChi-squared: 499.132, df(74), p.value &lt; 0.001 \nNr obs: 87,535\n\n\n\nclf_mc3.plot(\n    plots=\"pred\", incl=[\"time_fct\", \"app\", \"imppat\", \"clpat\", \"ctrpat\"]\n)\n\n\n\n\n\n\n\n\n\n\nInterpretation\nIntroducing variables like time_fct and app into a logistic regression model with imppat, clpat, and ctrpat can alter prediction plots by changing the relationships between predictors and the outcome. These new variables may interact with existing ones, influencing both the direction and magnitude of effects. Additionally, they can act as confounder adjustments, revealing the true impact of imppat, clpat, and ctrpat on click likelihood by accounting for hidden biases. Conditioning effects may also emerge, as the behavior of existing predictors shifts in the presence of time_fct and app, leading to different probability estimates. Ultimately, these changes are reflected in prediction plots, which visually capture the refined relationships between predictors and the outcome, offering a clearer interpretation of the model‚Äôs dynamics.\n\n\n\nDecile Analysis\n\ntz_test =tz_gaming[tz_gaming[\"training\"] == \"test\"]\ntz_test[\"pred_logit_dec\"] = (tz_test.groupby(\"training\").pred_logit.transform(rsm.xtile, 10, rev=True))\nprint(tz_test.head())\n\n      training     inum click  time time_fct    app mobile_os  impup  clup  \\\n87535     test  I300002    no    21       21   app1   android   1458     3   \n87536     test  I300006    no     3        3  app40       ios      3     0   \n87537     test  I300012    no     5        5  app12   android   5057     6   \n87538     test  I300015    no    10       10   app1   android   1993    10   \n87539     test  I300016    no    14       14   app1       ios    212     7   \n\n          ctrup  ...     ctrut  imppat  clpat    ctrpat       rnd  pred_vneta  \\\n87535  0.205761  ...  0.000000   68113    957  1.405018  0.147891    0.003961   \n87536  0.000000  ...  0.000000      50      0  0.000000  0.383246    0.018965   \n87537  0.118647  ...  0.000000     754      8  1.061008  1.274485    0.003961   \n87538  0.501756  ...  0.000000   26537    276  1.040057  0.673022    0.003961   \n87539  3.301887  ...  5.263158   57348    874  1.524029 -0.785851    0.050679   \n\n             id    pred_logit  pred_rnd  pred_logit_dec  \n87535  id466983  1.020981e-02  0.008791               4  \n87536  id946375  8.665095e-03  0.008718               4  \n87537  id479295  1.910723e-14  0.008448              10  \n87538   id83284  6.240407e-03  0.008630               5  \n87539  id359434  1.233449e-02  0.009086               3  \n\n[5 rows x 25 columns]\n\n\n/tmp/ipykernel_58081/1756994838.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tz_test[\"pred_logit_dec\"] = (tz_test.groupby(\"training\").pred_logit.transform(rsm.xtile, 10, rev=True))\n\n\n\ndec_tab = (\n    tz_test.groupby('pred_logit_dec')\n    .agg(\n        nr_impressions= (\"pred_logit_dec\", \"size\"),\n        nr_clicks= (\"click\", lambda x: (x == \"yes\").sum()),\n    )\n    .assign( ctr= lambda x: x.nr_clicks / x.nr_impressions)\n    .reset_index()\n)\ndec_tab\n\n\n\n\n\n\n\n\npred_logit_dec\nnr_impressions\nnr_clicks\nctr\n\n\n\n\n0\n1\n2796\n103\n0.036838\n\n\n1\n2\n2793\n48\n0.017186\n\n\n2\n3\n2788\n42\n0.015065\n\n\n3\n4\n2796\n30\n0.010730\n\n\n4\n5\n2802\n15\n0.005353\n\n\n5\n6\n2796\n7\n0.002504\n\n\n6\n7\n2794\n7\n0.002505\n\n\n7\n8\n2796\n3\n0.001073\n\n\n8\n9\n2796\n4\n0.001431\n\n\n9\n10\n2796\n12\n0.004292\n\n\n\n\n\n\n\n\nBar Chart\n\nimport matplotlib.pyplot as plt\nbc= dec_tab.plot.bar(x=\"pred_logit_dec\", y=\"ctr\", legend=False)\n\nbc.set_xlabel('Decile')\nbc.set_ylabel (\"click through rate (CTR)\")\nbc.set_title (\"Click Through Rate by Decile\")\nbc.axhline(dec_tab[\"ctr\"].mean(), color='r', linestyle='--')\n\nplt.show()\nprint(tz_train)\n\n\n\n\n\n\n\n\n      training     inum click  time time_fct    app mobile_os  impup  clup  \\\n0        train       I7    no     9        9   app8       ios    439     2   \n1        train      I23    no    15       15   app1       ios     64     0   \n2        train      I28    no    12       12   app5       ios     80     0   \n3        train      I30    no    19       19   app1       ios     25     0   \n4        train      I35    no    24       24   app1   android   3834    29   \n...        ...      ...   ...   ...      ...    ...       ...    ...   ...   \n87530    train  I299985    no    11       11   app2   android   1181     0   \n87531    train  I299986    no    10       10  app33       ios   1885     0   \n87532    train  I299990    no     1        1  app45       ios      8     0   \n87533    train  I299991    no     8        8   app1       ios    113     2   \n87534    train  I299995    no    18       18  app35   android     13     0   \n\n          ctrup  ...  clut     ctrut  imppat  clpat    ctrpat       rnd  \\\n0      0.455581  ...     0  0.000000      71      1  1.408451 -1.207066   \n1      0.000000  ...     0  0.000000   67312   1069  1.588127  0.277429   \n2      0.000000  ...     0  0.000000     331      1  0.302115  1.084441   \n3      0.000000  ...     0  0.000000   71114   1001  1.407599 -2.345698   \n4      0.756390  ...     4  1.215805  183852   2317  1.260253  0.429125   \n...         ...  ...   ...       ...     ...    ...       ...       ...   \n87530  0.000000  ...     0  0.000000    9625     14  0.145455 -0.249031   \n87531  0.000000  ...     0  0.000000     658      1  0.151976  0.770718   \n87532  0.000000  ...     0  0.000000     166      7  4.216867  0.181559   \n87533  1.769912  ...     0  0.000000   14245    158  1.109161 -1.263831   \n87534  0.000000  ...     1  7.142857     472      1  0.211864 -1.428302   \n\n       pred_vneta        id    pred_logit  pred_rnd  \n0        0.003961  id247135  3.382977e-13  0.009222  \n1        0.003961  id245079  1.156355e-02  0.008751  \n2        0.003961  id927245  2.655311e-03  0.008505  \n3        0.018965  id922188  1.349420e-02  0.009600  \n4        0.003961  id355833  1.868222e-03  0.008704  \n...           ...       ...           ...       ...  \n87530    0.003961  id565693  2.353185e-04  0.008915  \n87531    0.003961  id222657  8.101551e-04  0.008600  \n87532    0.018965  id340594  1.876022e-02  0.008781  \n87533    0.003961  id634151  9.397408e-03  0.009241  \n87534    0.050679  id280606  7.000647e-03  0.009294  \n\n[87535 rows x 24 columns]\n\n\n\n\n\nGain Curves\n\ndec_tab[\"cum_prop\"]= dec_tab[\"nr_impressions\"].cumsum()/dec_tab[\"nr_impressions\"].sum()\ndec_tab[\"cum_gains\"]= dec_tab['nr_clicks'].cumsum()/dec_tab['nr_clicks'].sum()\ngains_tab = dec_tab\ngains_tab\n\n\n\n\n\n\n\n\npred_logit_dec\nnr_impressions\nnr_clicks\nctr\ncum_prop\ncum_gains\n\n\n\n\n0\n1\n2796\n103\n0.036838\n0.100025\n0.380074\n\n\n1\n2\n2793\n48\n0.017186\n0.199943\n0.557196\n\n\n2\n3\n2788\n42\n0.015065\n0.299682\n0.712177\n\n\n3\n4\n2796\n30\n0.010730\n0.399707\n0.822878\n\n\n4\n5\n2802\n15\n0.005353\n0.499946\n0.878229\n\n\n5\n6\n2796\n7\n0.002504\n0.599971\n0.904059\n\n\n6\n7\n2794\n7\n0.002505\n0.699925\n0.929889\n\n\n7\n8\n2796\n3\n0.001073\n0.799950\n0.940959\n\n\n8\n9\n2796\n4\n0.001431\n0.899975\n0.955720\n\n\n9\n10\n2796\n12\n0.004292\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(dec_tab['cum_prop'], dec_tab['cum_gains'], label='Cumulative Gains', drawstyle='steps-post')\nplt.plot([0, 1], [0, 1], 'k--', label='No Model')  \n\n# Labeling the plot\nplt.title('Cumulative Gains Chart')\nplt.xlabel('Cumulative Proportion of Impressions')\nplt.ylabel('Cumulative Gains')\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\ncpm = 10 # cost per 1000 video impression\nconversion_rate= 0.05   # conversion to sign up with TZ after clicking on an ad\nclv= 25 # expected clv of customers that sign up with TZ after clicking on an ad\n\nthreshold= (cpm/(conversion_rate * clv *1000))\nthreshold\n\n0.008\n\n\n\ntz_test = tz_gaming[tz_gaming[\"training\"] == \"test\"].copy()\n\ntz_test[\"click_yes\"]= tz_test[\"click\"].apply(lambda x: 1 if x == \"yes\" else 0 if x == \"no\" else np.nan)\n\ntz_test[\"click_yes\"] = tz_test[\"click_yes\"].astype(float)\n\n\ntz_test\n\n\n\n\n\n\n\n\ntraining\ninum\nclick\ntime\ntime_fct\napp\nmobile_os\nimpup\nclup\nctrup\n...\nctrut\nimppat\nclpat\nctrpat\nrnd\npred_vneta\nid\npred_logit\npred_rnd\nclick_yes\n\n\n\n\n87535\ntest\nI300002\nno\n21\n21\napp1\nandroid\n1458\n3\n0.205761\n...\n0.000000\n68113\n957\n1.405018\n0.147891\n0.003961\nid466983\n1.020981e-02\n0.008791\n0.0\n\n\n87536\ntest\nI300006\nno\n3\n3\napp40\nios\n3\n0\n0.000000\n...\n0.000000\n50\n0\n0.000000\n0.383246\n0.018965\nid946375\n8.665095e-03\n0.008718\n0.0\n\n\n87537\ntest\nI300012\nno\n5\n5\napp12\nandroid\n5057\n6\n0.118647\n...\n0.000000\n754\n8\n1.061008\n1.274485\n0.003961\nid479295\n1.910723e-14\n0.008448\n0.0\n\n\n87538\ntest\nI300015\nno\n10\n10\napp1\nandroid\n1993\n10\n0.501756\n...\n0.000000\n26537\n276\n1.040057\n0.673022\n0.003961\nid83284\n6.240407e-03\n0.008630\n0.0\n\n\n87539\ntest\nI300016\nno\n14\n14\napp1\nios\n212\n7\n3.301887\n...\n5.263158\n57348\n874\n1.524029\n-0.785851\n0.050679\nid359434\n1.233449e-02\n0.009086\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n115483\ntest\nI399982\nno\n21\n21\napp2\nios\n2110\n0\n0.000000\n...\n0.000000\n23216\n19\n0.081840\n-1.852059\n0.003961\nid847352\n1.093091e-03\n0.009435\n0.0\n\n\n115484\ntest\nI399986\nno\n17\n17\napp14\nandroid\n291\n1\n0.343643\n...\n1.351351\n3665\n14\n0.381992\n-0.296415\n0.003961\nid457437\n3.609483e-03\n0.008930\n0.0\n\n\n115485\ntest\nI399991\nno\n23\n23\napp1\nandroid\n364\n3\n0.824176\n...\n0.000000\n173353\n2292\n1.322158\n0.099201\n0.003961\nid792352\n2.052670e-02\n0.008806\n0.0\n\n\n115486\ntest\nI399992\nno\n20\n20\napp6\nandroid\n59\n2\n3.389831\n...\n2.702703\n3474\n53\n1.525619\n-0.186421\n0.050679\nid115678\n2.192207e-02\n0.008896\n0.0\n\n\n115487\ntest\nI399994\nno\n18\n18\napp1\nios\n498\n7\n1.405622\n...\n1.886792\n77884\n1201\n1.542037\n0.857281\n0.003961\nid705546\n1.170346e-02\n0.008574\n0.0\n\n\n\n\n27953 rows √ó 25 columns\n\n\n\n\nTP = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_logit\"]&gt; threshold) & (tz_test[\"click_yes\"] == 1)).sum()\nFP = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_logit\"]&gt; threshold) & (tz_test[\"click_yes\"] == 0)).sum()\nTN = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_logit\"]&lt;= threshold) & (tz_test[\"click_yes\"] == 0)).sum()\nFN = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_logit\"]&lt;= threshold) & (tz_test[\"click_yes\"] == 1)).sum()\n\ncm_logit = pd.DataFrame(\n    {\n        \"label\": [\"TP\", \"FP\", \"TN\", \"FN\"],\n        \"nr\" : [TP, FP, TN, FN]# TP, FP, TN, and FN values in that order\n    }\n)\ncm_logit\n\n\n\n\n\n\n\n\nlabel\nnr\n\n\n\n\n0\nTP\n221\n\n\n1\nFP\n10661\n\n\n2\nTN\n17021\n\n\n3\nFN\n50\n\n\n\n\n\n\n\n\nAccuracy\n\naccuracy_logit = (TP + TN) / (TP + TN + FP + FN)# float\naccuracy_logit\n\nnp.float64(0.6168210925482059)\n\n\n\n\nConfusion Matrix Based on Pred_RND\n\nTP = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_rnd\"]&gt; threshold) & (tz_test[\"click_yes\"] == 1)).sum()\nFP = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_rnd\"]&gt; threshold) & (tz_test[\"click_yes\"] == 0)).sum()\nTN = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_rnd\"]&lt;= threshold) & (tz_test[\"click_yes\"] == 0)).sum()\nFN = ((tz_test[\"training\"] == \"test\") & (tz_test[\"pred_rnd\"]&lt;= threshold) & (tz_test[\"click_yes\"] == 1)).sum()\n\n\ncm_rnd = pd.DataFrame(\n    {\n        \"label\": [\"TP\", \"FP\", \"TN\", \"FN\"],\n        \"nr\": [TP, FP, TN, FN]# TP, FP, TN, and FN values in that order\n    }\n)\ncm_rnd\n\n\n\n\n\n\n\n\nlabel\nnr\n\n\n\n\n0\nTP\n271\n\n\n1\nFP\n27606\n\n\n2\nTN\n76\n\n\n3\nFN\n0\n\n\n\n\n\n\n\n\n\n\nAccuracy Based on Pred_RND Confusion Matrix\n\naccuracy_rnd = (TP + TN) / (TP + TN + FP + FN)# float\naccuracy_rnd\n\nnp.float64(0.012413694415626229)\n\n\n\nSummary and Interpretation\n\nfirst_model = {\n    \"TP\": 271,\n    \"FP\": 27606,\n    \"TN\": 76,\n    \"FN\": 0\n}\n\nrnd_model = {\n    \"TP\": 0,\n    \"FP\": 0,\n    \"TN\": 27682,\n    \"FN\": 271\n}\n\ndef compute_metrics(cm):\n    TP = cm[\"TP\"]\n    FP = cm[\"FP\"]\n    TN = cm[\"TN\"]\n    FN = cm[\"FN\"]\n    total = TP + FP + TN + FN\n\n    accuracy = (TP + TN) / total\n    precision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\n    specificity = TN / (TN + FP) if (TN + FP) &gt; 0 else 0\n    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n    return {\n        \"TP\": TP,\n        \"FP\": FP,\n        \"TN\": TN,\n        \"FN\": FN,\n        \"Accuracy\": round(accuracy, 4),\n        \"Precision\": round(precision, 4),\n        \"Recall\": round(recall, 4),\n        \"Specificity\": round(specificity, 4),\n        \"F1 Score\": round(f1_score, 4)\n    }\n\nfirst_metrics = compute_metrics(first_model)\nrnd_metrics = compute_metrics(rnd_model)\n\nresults_df = pd.DataFrame({\n    \"Metric\": list(first_metrics.keys()),\n    \"First Model\": list(first_metrics.values()),\n    \"RND\": list(rnd_metrics.values())\n})\n\nprint(results_df.to_string(index=False))\n\n     Metric  First Model        RND\n         TP     271.0000     0.0000\n         FP   27606.0000     0.0000\n         TN      76.0000 27682.0000\n         FN       0.0000   271.0000\n   Accuracy       0.0124     0.9903\n  Precision       0.0097     0.0000\n     Recall       1.0000     0.0000\nSpecificity       0.0027     1.0000\n   F1 Score       0.0193     0.0000\n\n\nThe First Model correctly identified all 271 true positives, achieving 100% recall but only 0.97% precision due to 27,606 false positives. In contrast, the RND model predicted no positives, resulting in perfect specificity and 99.02% accuracy, but 0% recall and no ability to detect actual clicks. Although the First Model‚Äôs accuracy was just 1.03%, it was more useful than the RND model because it captured all true click events. However, its extremely low precision highlights inefficiency, as most predicted clicks were incorrect. This comparison shows that in imbalanced datasets, metrics like precision, recall, and F1 score are more informative than accuracy alone\n\n\n\nModel Comparison\n\nCost Information\n\nCost per 1,000 video impressions (CPM) is $10\nConversion to sign-up as a TZ game player after clicking on an ad is 5%\nThe expected CLV of customers that sign-up with TZ after clicking on an ad is approximately $25\nThe total cost of the data from Vneta is $50K\nThe total cost charged for the data science consulting services by Vneta is $150K\n\n\ncpm = 10 # cost per 1000 video impression\nconversion_rate= 0.05   # conversion to sign up with TZ after clicking on an ad\nclv= 25 # expected clv of customers that sign up with TZ after clicking on an ad\ntotal_impressions = 20_000_000 # given in the problem above\nadditional_cost_VNETA = 50_000 # cost of the data from VNETA DATA\nadditional_cost_Consulting= 150_000 #cost of thr consulting services by VNETA\nadditional_cost_spamming= 0 #cost for spamming with no VNETA involve\n\ndef calculate_break_even_response_rate(cpm,conversion_rate, clv, total_impressions,additional_cost):\n    total_cost = (total_impressions/1000) *cpm +additional_cost\n    margin = total_impressions * conversion_rate * clv\n    break_even_rate_of_response= total_cost/margin\n    return break_even_rate_of_response\n\nbreak_even_response_rate_spamming= calculate_break_even_response_rate(cpm,conversion_rate, clv, total_impressions, additional_cost_spamming)\nbreak_even_response_rate_spamming\n\n0.008\n\n\n\ntz_gaming= tz_test\ntz_gaming[\"target_logit\"] = tz_gaming[\"pred_logit\"] &gt; break_even_response_rate_spamming\ntz_gaming[\"target_rnd\"] = tz_gaming[\"pred_rnd\"]&gt; break_even_response_rate_spamming\ntz_gaming[\"target_vneta\"]= tz_gaming[\"pred_vneta\"] &gt;break_even_response_rate_spamming\ntz_gaming\n\n\n\n\n\n\n\n\ntraining\ninum\nclick\ntime\ntime_fct\napp\nmobile_os\nimpup\nclup\nctrup\n...\nctrpat\nrnd\npred_vneta\nid\npred_logit\npred_rnd\nclick_yes\ntarget_logit\ntarget_rnd\ntarget_vneta\n\n\n\n\n87535\ntest\nI300002\nno\n21\n21\napp1\nandroid\n1458\n3\n0.205761\n...\n1.405018\n0.147891\n0.003961\nid466983\n1.020981e-02\n0.008791\n0.0\nTrue\nTrue\nFalse\n\n\n87536\ntest\nI300006\nno\n3\n3\napp40\nios\n3\n0\n0.000000\n...\n0.000000\n0.383246\n0.018965\nid946375\n8.665095e-03\n0.008718\n0.0\nTrue\nTrue\nTrue\n\n\n87537\ntest\nI300012\nno\n5\n5\napp12\nandroid\n5057\n6\n0.118647\n...\n1.061008\n1.274485\n0.003961\nid479295\n1.910723e-14\n0.008448\n0.0\nFalse\nTrue\nFalse\n\n\n87538\ntest\nI300015\nno\n10\n10\napp1\nandroid\n1993\n10\n0.501756\n...\n1.040057\n0.673022\n0.003961\nid83284\n6.240407e-03\n0.008630\n0.0\nFalse\nTrue\nFalse\n\n\n87539\ntest\nI300016\nno\n14\n14\napp1\nios\n212\n7\n3.301887\n...\n1.524029\n-0.785851\n0.050679\nid359434\n1.233449e-02\n0.009086\n0.0\nTrue\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n115483\ntest\nI399982\nno\n21\n21\napp2\nios\n2110\n0\n0.000000\n...\n0.081840\n-1.852059\n0.003961\nid847352\n1.093091e-03\n0.009435\n0.0\nFalse\nTrue\nFalse\n\n\n115484\ntest\nI399986\nno\n17\n17\napp14\nandroid\n291\n1\n0.343643\n...\n0.381992\n-0.296415\n0.003961\nid457437\n3.609483e-03\n0.008930\n0.0\nFalse\nTrue\nFalse\n\n\n115485\ntest\nI399991\nno\n23\n23\napp1\nandroid\n364\n3\n0.824176\n...\n1.322158\n0.099201\n0.003961\nid792352\n2.052670e-02\n0.008806\n0.0\nTrue\nTrue\nFalse\n\n\n115486\ntest\nI399992\nno\n20\n20\napp6\nandroid\n59\n2\n3.389831\n...\n1.525619\n-0.186421\n0.050679\nid115678\n2.192207e-02\n0.008896\n0.0\nTrue\nTrue\nTrue\n\n\n115487\ntest\nI399994\nno\n18\n18\napp1\nios\n498\n7\n1.405622\n...\n1.542037\n0.857281\n0.003961\nid705546\n1.170346e-02\n0.008574\n0.0\nTrue\nTrue\nFalse\n\n\n\n\n27953 rows √ó 28 columns\n\n\n\n\n\nSpamming\n\ntotal_succesful_clicks= tz_gaming[(tz_gaming[\"click_yes\"]== 1)].shape[0]\ntotal_succesful_clicks\ntotal_costs = (tz_gaming.shape[0]/1000)*cpm\ntotal_costs\ntotal_revenue= total_succesful_clicks*conversion_rate*clv\ntotal_revenue\nprofit_spam =total_revenue- total_costs\nprofit_spam\nrome_spam= (profit_spam/total_costs)\nrome_spam\nprint(profit_spam, rome_spam)\n\n59.22000000000003 0.21185561478195555\n\n\n\n\nLOGIT\n\ntotal_succesful_clicks = tz_gaming[(tz_gaming[\"target_logit\"] == True) & (tz_gaming[\"click_yes\"]== 1)].shape[0]\ntotal_succesful_clicks\ntotal_costs = (tz_gaming[(tz_gaming[\"target_logit\"]== True)].shape[0]/1000)*cpm\ntotal_costs\ntotal_revenue= total_succesful_clicks*conversion_rate*clv\ntotal_revenue\nprofit_logit =total_revenue- total_costs\nprofit_logit\nrome_logit= (profit_logit/total_costs)\nrome_logit\nprint(profit_logit, rome_logit)\n\n167.43 1.5385958463517737\n\n\n\n\nRND\n\ntotal_succesful_clicks = tz_gaming[(tz_gaming[\"target_rnd\"] == True) & (tz_gaming[\"click_yes\"]== 1)].shape[0]\ntotal_succesful_clicks\ntotal_costs = (tz_gaming[(tz_gaming[\"target_rnd\"]== True)].shape[0]/1000)*cpm\ntotal_costs\ntotal_revenue= total_succesful_clicks*conversion_rate*clv\ntotal_revenue\nprofit_rnd =total_revenue- total_costs\nprofit_rnd\nrome_rnd= (profit_rnd/total_costs)\nrome_rnd\nprint(profit_rnd, rome_rnd)\n\n59.98000000000002 0.21515945044301762\n\n\n\n\nVNETA\n\ntotal_succesful_clicks = tz_gaming[(tz_gaming[\"target_vneta\"] == True) & (tz_gaming[\"click_yes\"]== 1)].shape[0]\ntotal_succesful_clicks\ntotal_costs = (tz_gaming[(tz_gaming[\"target_vneta\"]== True)].shape[0]/1000)*cpm\ntotal_costs\ntotal_revenue= total_succesful_clicks*conversion_rate*clv\ntotal_revenue\nprofit_vneta =total_revenue- total_costs\nprofit_vneta\nrome_vneta= (profit_vneta/total_costs)\nrome_vneta\nprint(profit_vneta, rome_vneta)\n\n151.29 3.1059330732909047\n\n\n\n\nSummary\n\ntz_gaming[\"pred_spam\"] = 1\ntz_gaming[\"target_spam\"] = True\n\nmod_perf = pd.DataFrame(\n    {\n        \"model\": [\n            \"logit\",\n            \"rnd\",\n            \"vneta\",\n            \"spam\",\n        ],\n        \"profit\": [profit_logit, profit_rnd, profit_vneta, profit_spam],\n        \"ROME\": [rome_logit, rome_rnd, rome_vneta, rome_spam],\n    }\n)\nmod_perf\n\n\n\n\n\n\n\n\nmodel\nprofit\nROME\n\n\n\n\n0\nlogit\n167.43\n1.538596\n\n\n1\nrnd\n59.98\n0.215159\n\n\n2\nvneta\n151.29\n3.105933\n\n\n3\nspam\n59.22\n0.211856\n\n\n\n\n\n\n\n\n\nAnalysis\nThe Vneta model demonstrates the highest ROME at 3.105933, making it the most efficient in marketing budget utilization. It generates a profit of 151.29, slightly lower than the Logit model but still highly effective. This model is ideal for scenarios where marketing efficiency and budget allocation are top priorities. On the other hand, the Logit model achieves the highest profit of 167.43, though its ROME is lower than that of the Vneta model, indicating reduced spending efficiency. It is best suited for cases where maximizing total profit is more important than efficiency. In contrast, both the Rnd and Spam models underperform in terms of profit and ROME compared to predictive models, suggesting that predictive strategies significantly outperform non-targeted methods. Overall, the Vneta model is recommended for its superior marketing efficiency, while the Logit model is preferred for maximizing profit. The choice between these two depends on whether ROI or total profit is the primary objective.\n\n\n\nProfit Comparison\n\ntotal_impressions_purchased = 20_000_000\ntest_sample= tz_gaming.shape[0]\ntest_sample\nlogit = (profit_logit /test_sample)* total_impressions_purchased\nrnd= (profit_rnd/test_sample) * total_impressions_purchased\nvneta= (profit_vneta/test_sample) * total_impressions_purchased\nspam= (profit_spam/test_sample) * total_impressions_purchased\n\nlogit, rnd, vneta, spam\n\n(119793.93982756771, 42914.89285586521, 108245.98433084105, 42371.12295639111)\n\n\n\n\nROME Comparison\n\ntotal_costs = (tz_gaming[(tz_gaming[\"target_logit\"]== True)].shape[0]/1000)*cpm\nrome_logit = logit/total_costs\n\ntotal_costs = (tz_gaming[(tz_gaming[\"target_rnd\"]== True)].shape[0]/1000)*cpm\nrome_rnd= logit/total_costs\n\ntotal_costs = (tz_gaming[(tz_gaming[\"target_vneta\"]== True)].shape[0]/1000)*cpm\nrome_vneta = vneta/total_costs\n\ntotal_costs = (tz_gaming.shape[0]/1000)*cpm\nrome_spam = spam/total_costs \n\nrome_logit, rome_rnd, rome_vneta, rome_logit\n\n(1100.8448798710506, 429.7232120657449, 2222.2538355746465, 1100.8448798710506)\n\n\n\nProfit and ROME Summary\n\nmod_perf_20M = pd.DataFrame(\n    {\n        \"model\": [\n            \"logit\",\n            \"rnd\",\n            \"vneta\",\n            \"spam\",\n        ],\n        \"profit\": [logit, rnd, vneta, spam],\n        \"ROME\": [rome_logit, rome_rnd, rome_vneta, rome_spam],\n    }\n)\nmod_perf_20M\n\n\n\n\n\n\n\n\nmodel\nprofit\nROME\n\n\n\n\n0\nlogit\n119793.939828\n1100.844880\n\n\n1\nrnd\n42914.892856\n429.723212\n\n\n2\nvneta\n108245.984331\n2222.253836\n\n\n3\nspam\n42371.122956\n151.579877"
  },
  {
    "objectID": "projects/hw1/index1.html#conclusion",
    "href": "projects/hw1/index1.html#conclusion",
    "title": "TZ Gaming: Optimal Targeting of Mobile Ads",
    "section": "Conclusion",
    "text": "Conclusion\nThe Logit model leads in profit generation with $119,793.94, making it the top choice for revenue maximization, though its ROME stands at 1100.84, which, while strong, is not the highest. The Rnd model lags behind with a profit of $42,914.89 and a ROME of 429.73, reflecting lower efficiency and profitability. Similarly, the Spam model records $42,371.12 in profit, closely matching the Rnd model, but has the lowest ROME at 151.579877, making it the least effective approach. The Vneta model, on the other hand, secures a high profit of $108,245.98 and boasts the highest ROME at 2222.253, demonstrating superior marketing efficiency. This makes Vneta the optimal choice for balancing profit and marketing spend efficiency. Given its outstanding ROME, the Vneta model is the primary recommendation for maximizing investment returns. However, the Logit model remains a solid alternative for cases where absolute profit takes precedence over efficiency. The new data aligns with previous recommendations, reaffirming Vneta‚Äôs leadership in efficiency despite slightly lower profits than the Logit model. The results confirm that predictive models significantly outperform non-targeted approaches like the Rnd and Spam models. Ultimately, the Vneta model is best for those prioritizing a balance of high profit and peak efficiency, while the Logit model is the go-to option for pure profit maximization."
  }
]