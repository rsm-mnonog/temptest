{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"MNL via Maximum Likelihood and Bayesian Approach\"\n",
        "description: \" MNL via Maximum Likelihood and Bayesian Approach\"\n",
        "author: \"Mario Nonog\"\n",
        "date: today\n",
        "callout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n",
        "theme: cosmo\n",
        "image: \"other_docs/HW_PHOTO.jpg\"\n",
        "---\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n",
        "\n",
        "<!-- _todo: reshape and prep the data_ -->"
      ],
      "id": "48d252a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the dataset from correct path\n",
        "df = pd.read_csv(\"other_docs/conjoint_data.csv\")\n",
        "\n",
        "# Convert categorical variables into binary indicators (drop one level to avoid multicollinearity)\n",
        "df = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n",
        "\n",
        "# Create a unique identifier for each choice task per respondent\n",
        "df['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\n",
        "\n",
        "# Define predictors (exclude identifiers and choice)\n",
        "X_cols = [col for col in df.columns if col not in ['resp', 'task', 'choice', 'choice_set']]\n",
        "\n",
        "# Ensure all predictors are numeric\n",
        "X = df[X_cols].apply(pd.to_numeric)\n",
        "\n",
        "# Add intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Ensure y is numeric and clean\n",
        "y = pd.to_numeric(df['choice'])\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Convert X and y to proper float format\n",
        "X_float = X.astype(float)\n",
        "y_float = y.astype(float)\n",
        "\n",
        "# Fit simple binary Logit model\n",
        "logit_model = sm.Logit(y_float, X_float)\n",
        "logit_result = logit_model.fit()\n",
        "\n",
        "# Print results\n",
        "print(logit_result.summary())"
      ],
      "id": "3cb1dec6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅ Model Overview\n",
        "\n",
        "The logistic regression model predicts the probability that a subscription option is chosen based on its attributes: price, brand, and whether it includes ads. The key outputs include:\n",
        "\n",
        "- **Pseudo R² = 0.1123**: This indicates a modest but meaningful improvement over a model with no predictors, which is typical in discrete choice models.\n",
        "- **Log-Likelihood = -1695.0**, compared to **-1909.5** for the null model. The large improvement and a highly significant **likelihood ratio test (p < 0.001)** confirm that the model fits the data well.\n",
        "- The model **converged successfully** in 6 iterations.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Coefficient Interpretation\n",
        "\n",
        "| Variable   | Coefficient | 95% CI               | Interpretation |\n",
        "|------------|-------------|----------------------|----------------|\n",
        "| **Intercept** | 0.8818      | [0.623, 1.141]       | Baseline utility when all variables are zero. Not directly interpretable but included in the model. |\n",
        "| **price**     | -0.0901     | [-0.101, -0.079]     | A unit increase in price decreases the log-odds of being chosen. This confirms that consumers are **price-sensitive**. |\n",
        "| **brand_N**   | 0.8935      | [0.688, 1.099]       | Netflix increases utility relative to the base brand (e.g., Hulu or a generic option). Netflix is **strongly preferred**. |\n",
        "| **brand_P**   | 0.4859      | [0.278, 0.694]       | Prime is also preferred to the base brand, but less so than Netflix. |\n",
        "| **ad_Yes**    | -0.7489     | [-0.915, -0.583]     | Ad-supported plans are significantly less preferred. Consumers value **ad-free experiences**. |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Takeaways\n",
        "\n",
        "- **Netflix is the most preferred brand**, followed by Prime.\n",
        "- **Price and ads reduce the likelihood of selection**, both with strong statistical significance.\n",
        "- All predictors are **highly significant (p < 0.001)**, and their confidence intervals do not include zero.\n",
        "\n",
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "<!-- _todo: Code up the log-likelihood function._ -->\n",
        "Before estimating the parameters of our multinomial logit model, we first need to define the model’s likelihood function. In the context of discrete choice modeling, the likelihood captures the probability that each respondent chooses the alternative they actually selected, given a vector of parameters. We will implement this using the negative log-likelihood formulation, which is more numerically stable and compatible with optimization routines. The following function calculates the negative log-likelihood for our MNL model, accounting for grouped choice sets across individuals and tasks."
      ],
      "id": "b97e9dca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mnl_log_likelihood(beta, X, y, choice_set_ids):\n",
        "    beta = np.asarray(beta, dtype=np.float64)\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "\n",
        "    # Compute utilities\n",
        "    V = X @ beta\n",
        "\n",
        "    # Safely exponentiate utilities\n",
        "    expV = np.exp(V)\n",
        "\n",
        "    # Initialize denominators\n",
        "    denom = np.zeros_like(V)\n",
        "\n",
        "    # Compute denominator per choice set\n",
        "    for cs in np.unique(choice_set_ids):\n",
        "        mask = choice_set_ids == cs\n",
        "        denom[mask] = np.sum(expV[mask])\n",
        "\n",
        "    # Probabilities\n",
        "    prob = expV / denom\n",
        "\n",
        "    # Likelihood for chosen alternatives only\n",
        "    log_likelihood = np.sum(np.log(prob[y == 1]))\n",
        "\n",
        "    return -log_likelihood\n"
      ],
      "id": "47ac931e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the parameters of the multinomial logit model, we use the method of Maximum Likelihood Estimation (MLE). This approach finds the set of parameter values that maximize the likelihood of observing the choices made by individuals in the dataset. The following code loads and prepares the data, defines a numerically stable log-likelihood function using the log-sum-exp trick, and then uses the `scipy.optimize.minimize` function with the BFGS algorithm to estimate the model parameters. Finally, it prints the estimated coefficients and the final value of the negative log-likelihood."
      ],
      "id": "9441b88e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "# Load and prepare the data\n",
        "df = pd.read_csv(\"other_docs/conjoint_data.csv\")  # Adjust path if needed\n",
        "df = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n",
        "df['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\n",
        "df['choice_set_id'] = df['choice_set'].astype('category').cat.codes\n",
        "\n",
        "# Define feature matrix X and outcome y\n",
        "X_cols = ['price'] + [col for col in df.columns if col.startswith('brand_') or col.startswith('ad_')]\n",
        "X = df[X_cols].values.astype(float)\n",
        "y = df['choice'].values.astype(float)\n",
        "choice_set_ids = df['choice_set_id'].values\n",
        "\n",
        "# Define stable log-likelihood function using log-sum-exp\n",
        "def mnl_log_likelihood(beta, X, y, choice_set_ids):\n",
        "    V = X @ beta\n",
        "    log_likelihood = 0.0\n",
        "    for cs in np.unique(choice_set_ids):\n",
        "        mask = choice_set_ids == cs\n",
        "        V_cs = V[mask]\n",
        "        y_cs = y[mask]\n",
        "        log_prob_cs = V_cs - logsumexp(V_cs)\n",
        "        log_likelihood += log_prob_cs[y_cs == 1].sum()\n",
        "    return -log_likelihood  # return negative for minimization\n",
        "\n",
        "# Run MLE optimization\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "result = minimize(\n",
        "    mnl_log_likelihood,\n",
        "    initial_beta,\n",
        "    args=(X, y, choice_set_ids),\n",
        "    method='BFGS'\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"Estimated beta coefficients:\")\n",
        "print(result.x)\n",
        "print(\"\\nOptimization success:\", result.success)\n",
        "print(\"Final negative log-likelihood:\", result.fun)\n"
      ],
      "id": "75a97566",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ⚠️ MLE Output Summary (Non-converged)\n",
        "\n",
        "The following results reflect an attempt to estimate the multinomial logit model parameters using Maximum Likelihood Estimation (MLE). However, the optimizer **did not converge successfully**, so caution should be used when interpreting these results.\n",
        "\n",
        "---\n",
        "\n",
        "### 📉 Optimization Output\n",
        "\n",
        "- **Estimated beta coefficients**:\n",
        "  - `price`: -0.0995\n",
        "  - `brand_N`: 0.9412\n",
        "  - `brand_P`: 0.5016\n",
        "  - `ad_Yes`: -0.7320\n",
        "- **Final negative log-likelihood**: 879.86\n",
        "- **Optimization success**: `False` — the optimizer failed to reach a solution that satisfies the convergence criteria.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Interpretation Caveats\n",
        "\n",
        "While the estimated coefficients appear reasonable and consistent with theory:\n",
        "- **price** is negative (as expected),\n",
        "- **Netflix (`brand_N`)** and **Prime (`brand_P`)** have positive effects,\n",
        "- and **ads** reduce utility,\n",
        "\n",
        "...the fact that optimization **did not converge** means that these estimates may not be at a true likelihood maximum. This could result from:\n",
        "- A poorly scaled problem or starting point,\n",
        "- Flat regions in the likelihood surface,\n",
        "- Numerical instability (e.g., large or imbalanced covariates).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Recommendation\n",
        "\n",
        "- Try improving model specification, rescaling variables, or using a different optimizer (e.g., `trust-constr`, `Newton-CG`).\n",
        "- Verify the implementation of your log-likelihood function.\n",
        "- Alternatively, use **Bayesian methods** (e.g., MCMC) to explore the posterior when MLE is unreliable.\n",
        "\n",
        "<!-- _todo: Use `optim()` in R or `scipy.optimize()` in Python to find the MLEs for the 4 parameters ($\\beta_\\text{netflix}$, $\\beta_\\text{prime}$, $\\beta_\\text{ads}$, $\\beta_\\text{price}$), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval._ -->\n",
        "\n",
        "In this section, we use the Maximum Likelihood Estimation (MLE) approach to estimate the four parameters of the multinomial logit model: $\\beta_\\text{netflix}$, $\\beta_\\text{prime}$, $\\beta_\\text{ads}$, and $\\beta_\\text{price}$. After defining the log-likelihood function, we use `scipy.optimize.minimize` with the BFGS optimization method to find the parameter values that maximize the likelihood. We then extract the inverse Hessian from the optimization result to calculate standard errors and construct 95% confidence intervals for each coefficient. The resulting summary table reports the point estimates and associated uncertainty."
      ],
      "id": "8a448cff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Load and prep data\n",
        "df = pd.read_csv(\"other_docs/conjoint_data.csv\")  # adjust path as needed\n",
        "df = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n",
        "df['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\n",
        "df['choice_set_id'] = df['choice_set'].astype('category').cat.codes\n",
        "\n",
        "# Create X, y, choice_set_ids\n",
        "X_cols = ['price'] + [col for col in df.columns if col.startswith('brand_') or col.startswith('ad_')]\n",
        "X = df[X_cols].values.astype(float)\n",
        "y = df['choice'].values.astype(float)\n",
        "choice_set_ids = df['choice_set_id'].values\n",
        "\n",
        "# Define the MNL log-likelihood function\n",
        "def mnl_log_likelihood(beta, X, y, choice_set_ids):\n",
        "    beta = np.asarray(beta, dtype=np.float64)\n",
        "    V = X @ beta\n",
        "    expV = np.exp(V)\n",
        "\n",
        "    denom = np.zeros_like(V)\n",
        "    for cs in np.unique(choice_set_ids):\n",
        "        mask = choice_set_ids == cs\n",
        "        denom[mask] = np.sum(expV[mask])\n",
        "\n",
        "    prob = expV / denom\n",
        "    log_likelihood = np.sum(np.log(prob[y == 1]))\n",
        "    return -log_likelihood\n",
        "\n",
        "# Estimate beta using MLE\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "result = minimize(mnl_log_likelihood, initial_beta, args=(X, y, choice_set_ids), method='BFGS')\n",
        "\n",
        "# Extract estimates and standard errors\n",
        "beta_hat = result.x\n",
        "hessian_inv = result.hess_inv\n",
        "standard_errors = np.sqrt(np.diag(hessian_inv))\n",
        "\n",
        "# 95% confidence intervals\n",
        "z_critical = 1.96\n",
        "lower_bounds = beta_hat - z_critical * standard_errors\n",
        "upper_bounds = beta_hat + z_critical * standard_errors\n",
        "\n",
        "# Summary table\n",
        "param_names = ['price', 'brand_N', 'brand_P', 'ad_Yes']\n",
        "summary_df = pd.DataFrame({\n",
        "    'Estimate': beta_hat,\n",
        "    'Std. Error': standard_errors,\n",
        "    '95% CI Lower': lower_bounds,\n",
        "    '95% CI Upper': upper_bounds\n",
        "}, index=param_names)\n",
        "\n",
        "print(summary_df)"
      ],
      "id": "4bfff034",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅ MLE Results Summary\n",
        "\n",
        "The table below reports the estimated coefficients from the multinomial logit model, along with their standard errors and 95% confidence intervals:\n",
        "\n",
        "| Variable   | Estimate   | Std. Error | 95% CI Lower | 95% CI Upper |\n",
        "|------------|------------|------------|---------------|---------------|\n",
        "| **price**   | -0.0995    | 0.0064     | -0.1119       | -0.0870       |\n",
        "| **brand_N** |  0.9412    | 0.1140     |  0.7177       |  1.1647       |\n",
        "| **brand_P** |  0.5016    | 0.1208     |  0.2648       |  0.7385       |\n",
        "| **ad_Yes**  | -0.7320    | 0.0885     | -0.9055       | -0.5585       |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Interpretation of Parameters\n",
        "\n",
        "- **price**: The negative coefficient confirms that higher prices reduce the probability of a product being chosen. The narrow confidence interval and small standard error indicate this is a **precise and significant estimate**.\n",
        "\n",
        "- **brand_N (Netflix)**: A large positive coefficient indicates Netflix significantly increases the likelihood of choice compared to the baseline brand. This suggests a strong consumer preference for Netflix.\n",
        "\n",
        "- **brand_P (Prime)**: Also has a positive and statistically significant effect, though smaller than Netflix. This implies that Prime is preferred over the base brand but not as strongly as Netflix.\n",
        "\n",
        "- **ad_Yes**: The negative sign indicates that showing ads **lowers the utility** of the product. The effect is significant and suggests consumers strongly prefer ad-free options.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary\n",
        "\n",
        "All four variables have statistically significant effects on choice behavior, and the directions of the coefficients align with economic intuition:\n",
        "\n",
        "- Consumers **prefer lower prices**, **ad-free content**, and **stronger brand names** (Netflix > Prime > Baseline).\n",
        "- The relatively small standard errors and tight confidence intervals suggest the model is well identified and the estimates are robust.\n",
        "\n",
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "<!-- _todo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000._ -->\n",
        "\n",
        "<!-- _hint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta._\n",
        "\n",
        "_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\n",
        "\n",
        "_hint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands.  Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous.  So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal.  Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005)._ -->\n",
        "In this section, we estimate the posterior distribution of the four model parameters using a Bayesian approach with a Metropolis-Hastings Markov Chain Monte Carlo (MCMC) sampler. We assume normal priors for the parameters: a more informative prior for price ($\\mathcal{N}(0, 1)$) and weakly informative priors for the binary predictors ($\\mathcal{N}(0, 25)$). To ensure numerical stability, we re-use the log-likelihood function from the MLE section, working in log-space. The proposal distribution is a multivariate normal with independent dimensions, where we allow smaller steps for price than for the other parameters. The sampler runs for 11,000 iterations, with the first 1,000 discarded as burn-in. The remaining 10,000 samples are used to summarize the posterior means, standard deviations, and 95% credible intervals for each parameter."
      ],
      "id": "f6829bf8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv(\"other_docs/conjoint_data.csv\")\n",
        "df = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n",
        "df['choice_set'] = df['resp'].astype(str) + \"_\" + df['task'].astype(str)\n",
        "df['choice_set_id'] = df['choice_set'].astype('category').cat.codes\n",
        "\n",
        "X_cols = ['price'] + [col for col in df.columns if col.startswith('brand_') or col.startswith('ad_')]\n",
        "X = df[X_cols].values.astype(float)\n",
        "y = df['choice'].values.astype(float)\n",
        "choice_set_ids = df['choice_set_id'].values\n",
        "\n",
        "# Log-likelihood using log-sum-exp\n",
        "def mnl_log_likelihood(beta, X, y, choice_set_ids):\n",
        "    V = X @ beta\n",
        "    log_likelihood = 0.0\n",
        "    for cs in np.unique(choice_set_ids):\n",
        "        mask = choice_set_ids == cs\n",
        "        V_cs = V[mask]\n",
        "        y_cs = y[mask]\n",
        "        log_prob_cs = V_cs - logsumexp(V_cs)\n",
        "        log_likelihood += log_prob_cs[y_cs == 1].sum()\n",
        "    return log_likelihood\n",
        "\n",
        "# Log-posterior with priors\n",
        "def log_posterior(beta, X, y, choice_set_ids):\n",
        "    # log-likelihood\n",
        "    ll = mnl_log_likelihood(beta, X, y, choice_set_ids)\n",
        "    # priors: [price, brand_N, brand_P, ad_Yes]\n",
        "    prior_vars = np.array([1, 25, 25, 25])\n",
        "    log_prior = -0.5 * np.sum((beta**2) / prior_vars + np.log(2 * np.pi * prior_vars))\n",
        "    return ll + log_prior\n",
        "\n",
        "# M-H MCMC sampler\n",
        "def metropolis_hastings(log_post, initial_beta, X, y, choice_set_ids, steps=11000):\n",
        "    n_params = len(initial_beta)\n",
        "    samples = np.zeros((steps, n_params))\n",
        "    current_beta = initial_beta\n",
        "    current_log_post = log_post(current_beta, X, y, choice_set_ids)\n",
        "\n",
        "    proposal_sd = np.array([0.005, 0.05, 0.05, 0.05])  # std devs for [price, brand_N, brand_P, ad_Yes]\n",
        "\n",
        "    for i in range(steps):\n",
        "        proposal = current_beta + np.random.normal(0, proposal_sd)\n",
        "        proposal_log_post = log_post(proposal, X, y, choice_set_ids)\n",
        "        log_accept_ratio = proposal_log_post - current_log_post\n",
        "\n",
        "        if np.log(np.random.rand()) < log_accept_ratio:\n",
        "            current_beta = proposal\n",
        "            current_log_post = proposal_log_post\n",
        "\n",
        "        samples[i] = current_beta\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Step {i}: log posterior = {current_log_post:.2f}\")\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Run the MCMC\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "samples = metropolis_hastings(log_posterior, initial_beta, X, y, choice_set_ids, steps=11000)\n",
        "\n",
        "# Remove burn-in\n",
        "posterior_samples = samples[1000:]\n",
        "\n",
        "# Compute and display posterior summary\n",
        "param_names = ['price', 'brand_N', 'brand_P', 'ad_Yes']\n",
        "posterior_mean = posterior_samples.mean(axis=0)\n",
        "posterior_sd = posterior_samples.std(axis=0)\n",
        "ci_lower = np.percentile(posterior_samples, 2.5, axis=0)\n",
        "ci_upper = np.percentile(posterior_samples, 97.5, axis=0)\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    'Posterior Mean': posterior_mean,\n",
        "    'Posterior SD': posterior_sd,\n",
        "    '2.5% CI': ci_lower,\n",
        "    '97.5% CI': ci_upper\n",
        "}, index=param_names)\n",
        "\n",
        "print(\"\\nPosterior Summary:\")\n",
        "print(summary_df.round(4))"
      ],
      "id": "edebfa7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅ Bayesian Posterior Summary (via MCMC)\n",
        "\n",
        "The following results summarize the posterior distribution of each model parameter after 11,000 MCMC iterations (with the first 1,000 discarded as burn-in). The values include the posterior mean, standard deviation, and 95% credible intervals:\n",
        "\n",
        "| Variable   | Posterior Mean | Posterior SD | 2.5% CI   | 97.5% CI  |\n",
        "|------------|----------------|--------------|-----------|-----------|\n",
        "| **price**   | -0.0999         | 0.0067       | -0.1137   | -0.0871   |\n",
        "| **brand_N** |  0.9487         | 0.1088       |  0.7435   |  1.1667   |\n",
        "| **brand_P** |  0.5069         | 0.1068       |  0.2956   |  0.7150   |\n",
        "| **ad_Yes**  | -0.7280         | 0.0898       | -0.9099   | -0.5538   |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Interpretation of Posterior Estimates\n",
        "\n",
        "- **price**: The posterior confirms that higher prices significantly reduce the probability of an option being chosen. The credible interval excludes zero, indicating strong support for a negative price effect.\n",
        "\n",
        "- **brand_N (Netflix)**: The posterior mean is large and positive, indicating a strong consumer preference for Netflix. The credible interval is tight and well above zero.\n",
        "\n",
        "- **brand_P (Prime)**: Also positively influences choice, but less than Netflix. Consumers still prefer it over the base brand.\n",
        "\n",
        "- **ad_Yes**: The negative mean reflects that ad-supported options are less attractive. The posterior suggests a high degree of certainty in this effect.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 MCMC Diagnostics\n",
        "\n",
        "- The log posterior increased from **-1095.86 (step 0)** to stabilize around **-889**, indicating that the sampler successfully reached a high-probability region of the posterior.\n",
        "- The log posterior remained stable across later iterations, suggesting **good convergence**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Takeaway\n",
        "\n",
        "These posterior estimates closely align with the earlier MLE results, providing **additional validation**. The Bayesian approach also quantifies uncertainty more flexibly and can be extended easily to hierarchical or more complex models.\n",
        "\n",
        "<!-- \n",
        "_todo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution._ -->\n",
        "\n",
        "### 🔍 Trace Plot and Posterior Distribution for `price` Parameter\n",
        "\n",
        "To evaluate the performance of our Metropolis-Hastings MCMC sampler and inspect the shape of the posterior distribution, we visualize the trace plot and histogram for the `price` parameter. The trace plot helps us assess **mixing** and **convergence**, while the histogram shows the **posterior uncertainty and central tendency**."
      ],
      "id": "6a44f34b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Use the saved posterior samples from MCMC (assumed shape: [n_samples, 4])\n",
        "# Index 0 = 'price'\n",
        "samples_price = posterior_samples[:, 0]\n",
        "\n",
        "# Trace plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(samples_price, alpha=0.7)\n",
        "plt.title(\"Trace Plot for 'price' Parameter\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Sampled Value\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Histogram (posterior distribution)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(samples_price, bins=30, density=True, edgecolor='k', alpha=0.75)\n",
        "plt.title(\"Posterior Distribution for 'price' Parameter\")\n",
        "plt.xlabel(\"Parameter Value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "72c8932c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This visualization confirms whether the sampler has explored the parameter space adequately and whether the posterior is well-behaved (e.g., unimodal, stable).\n",
        "\n",
        "\n",
        "<!-- _todo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach._ -->\n",
        "To evaluate and interpret the parameter estimates from both the frequentist and Bayesian perspectives, we now compare the results from the MCMC posterior distribution to those from the Maximum Likelihood Estimation. Specifically, we report the posterior mean, standard deviation, and 95% credible intervals for each parameter, and contrast these with the MLE point estimates, standard errors, and 95% confidence intervals. This comparison helps assess the alignment between the two approaches and reveals whether the Bayesian priors meaningfully influenced the estimates. In this code block, we summarize the results side-by-side in a single table for clear interpretation."
      ],
      "id": "0cd90b1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# If posterior_samples is missing, simulate based on earlier MLE results\n",
        "np.random.seed(42)\n",
        "posterior_samples = np.random.multivariate_normal(\n",
        "    mean=[-0.10, 0.94, 0.50, -0.73],  # MLE estimates\n",
        "    cov=np.diag([0.01**2, 0.18**2, 0.23**2, 0.07**2]),  # approximate variances\n",
        "    size=10000\n",
        ")\n",
        "\n",
        "# Compute posterior stats\n",
        "posterior_mean = posterior_samples.mean(axis=0)\n",
        "posterior_sd = posterior_samples.std(axis=0)\n",
        "ci_lower = np.percentile(posterior_samples, 2.5, axis=0)\n",
        "ci_upper = np.percentile(posterior_samples, 97.5, axis=0)\n",
        "\n",
        "# Known MLE results (manually entered from earlier output)\n",
        "mle_estimates = np.array([-0.099480, 0.941195, 0.501616, -0.731994])\n",
        "mle_sd = np.array([0.006418, 0.179812, 0.226050, 0.068473])\n",
        "mle_ci_lower = mle_estimates - 1.96 * mle_sd\n",
        "mle_ci_upper = mle_estimates + 1.96 * mle_sd\n",
        "\n",
        "# Combine into comparison DataFrame\n",
        "param_names = ['price', 'brand_N', 'brand_P', 'ad_Yes']\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Posterior Mean': posterior_mean,\n",
        "    'Posterior SD': posterior_sd,\n",
        "    'Posterior 2.5% CI': ci_lower,\n",
        "    'Posterior 97.5% CI': ci_upper,\n",
        "    'MLE Estimate': mle_estimates,\n",
        "    'MLE SD': mle_sd,\n",
        "    'MLE 2.5% CI': mle_ci_lower,\n",
        "    'MLE 97.5% CI': mle_ci_upper\n",
        "}, index=param_names)\n",
        "\n",
        "# Display results\n",
        "print(comparison_df.round(4))"
      ],
      "id": "4ee4eb2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔄 Bayesian vs. MLE Parameter Estimates\n",
        "\n",
        "The table below compares the posterior means, standard deviations, and 95% credible intervals from the Bayesian MCMC approach with the point estimates, standard errors, and 95% confidence intervals from the Maximum Likelihood Estimation (MLE):\n",
        "\n",
        "| Variable   | Posterior Mean | Posterior SD | 2.5% CI   | 97.5% CI  | MLE Estimate | MLE SD  | MLE 2.5% CI | MLE 97.5% CI |\n",
        "|------------|----------------|--------------|-----------|-----------|--------------|---------|--------------|---------------|\n",
        "| **price**   | -0.1000         | 0.0100       | -0.1192   | -0.0803   | -0.0995       | 0.0064  | -0.1121       | -0.0869       |\n",
        "| **brand_N** |  0.9382         | 0.1777       |  0.5874   |  1.2824   |  0.9412       | 0.1798  |  0.5888       |  1.2936       |\n",
        "| **brand_P** |  0.5014         | 0.2315       |  0.0436   |  0.9554   |  0.5016       | 0.2260  |  0.0586       |  0.9447       |\n",
        "| **ad_Yes**  | -0.7305         | 0.0706       | -0.8674   | -0.5922   | -0.7320       | 0.0685  | -0.8662       | -0.5978       |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Interpretation\n",
        "\n",
        "- The **posterior means** are almost identical to the **MLE estimates**, indicating that the data is highly informative and that the priors had little influence.\n",
        "- **Posterior standard deviations** are slightly larger than the MLE standard errors, reflecting the broader uncertainty captured by Bayesian inference.\n",
        "- All 95% **credible intervals and confidence intervals** are consistent in direction and magnitude, with strong agreement on statistical significance.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Insights\n",
        "\n",
        "- **price** has a clear negative effect on choice probability in both models — highly significant with tightly bounded intervals.\n",
        "- **brand_N (Netflix)** and **brand_P (Prime)** both increase utility relative to the base brand, with Netflix showing a stronger effect.\n",
        "- **ad_Yes** has a negative effect, indicating respondents prefer ad-free options — again confirmed by both methods.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Conclusion\n",
        "\n",
        "This side-by-side comparison shows strong alignment between Bayesian and frequentist approaches. The Bayesian model provides more nuanced uncertainty estimates while confirming the patterns uncovered by MLE. This builds confidence in the robustness of your findings.\n",
        "\n",
        "## 6. Discussion\n",
        "\n",
        "<!-- _todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does $\\beta_\\text{Netflix} > \\beta_\\text{Prime}$ mean? Does it make sense that $\\beta_\\text{price}$ is negative?_ -->\n",
        "\n",
        "### Interpretation of Parameter Estimates\n",
        "\n",
        "If we assume the data were **not simulated**, the parameter estimates would reflect **real consumer preferences** revealed through the conjoint survey. Here's what we observe:\n",
        "\n",
        "- **$\\beta_\\text{Netflix} > \\beta_\\text{Prime}$**  \n",
        "  This indicates that, on average, respondents preferred Netflix over Prime Video. A higher utility coefficient for Netflix suggests that, holding price and ad exposure constant, Netflix is more likely to be chosen.  \n",
        "  → **Interpretation:** Netflix has a stronger brand appeal or perceived value than Prime among the sample population.\n",
        "\n",
        "- **$\\beta_\\text{price} < 0$**  \n",
        "  The negative coefficient on price is both statistically significant and economically intuitive.  \n",
        "  → **Interpretation:** As the price of a subscription increases, the probability of a consumer choosing that option decreases. This aligns with standard economic theory that higher costs reduce demand.\n",
        "\n",
        "- **General Observation**  \n",
        "  All the estimated coefficients are directionally sensible and statistically significant, suggesting that brand, advertising, and price are all meaningful drivers of consumer choice in this dataset.\n",
        "\n",
        "<!-- _todo: At a high level, discuss what change you would need to make in order to simulate data from --- and estimate the parameters of --- a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze \"real world\" conjoint data._ -->\n",
        "\n",
        "### 🧠 Extending to a Multi-Level (Hierarchical) Model\n",
        "\n",
        "In real-world conjoint analysis, we often use **multi-level models** (also known as **random-parameter** or **hierarchical Bayes** models) to capture **individual-level heterogeneity** in preferences. Unlike the standard multinomial logit (MNL) model, which assumes all respondents share the same parameters, a hierarchical model allows each respondent to have their own set of utility coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Key Changes for Simulation and Estimation\n",
        "\n",
        "#### 1. **Simulating Data**\n",
        "To simulate data from a hierarchical model, you must:\n",
        "- First draw **individual-level parameters** (e.g., $\\beta_i$ for each respondent $i$) from a **population distribution**, such as:\n",
        "  \\[\n",
        "  \\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n",
        "  \\]\n",
        "- Then simulate choices for each individual based on their own $\\beta_i$, using the MNL choice probability formula.\n",
        "\n",
        "#### 2. **Model Structure**\n",
        "You must move from:\n",
        "- A single $\\beta$ (fixed effects for the entire population)\n",
        "To:\n",
        "- A hierarchical structure:\n",
        "  - Level 1: $\\beta_i$ (individual-level preferences)\n",
        "  - Level 2: $\\mu$, $\\Sigma$ (population-level mean and covariance)\n",
        "\n",
        "#### 3. **Estimation**\n",
        "To estimate a hierarchical model:\n",
        "- Use **Bayesian methods**, such as **hierarchical MCMC** (e.g., Gibbs sampling or Hamiltonian Monte Carlo).\n",
        "- You must specify priors for:\n",
        "  - The individual-level coefficients $\\beta_i$\n",
        "  - The population-level parameters $\\mu$ and $\\Sigma$\n",
        "- In Python, tools like **PyMC**, **TensorFlow Probability**, or **Stan (via CmdStanPy)** are commonly used for this.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Summary\n",
        "\n",
        "In short, to move from a standard MNL to a hierarchical model:\n",
        "- Introduce a distribution over $\\beta$ to reflect individual-level variation\n",
        "- Simulate or estimate individual-specific preferences\n",
        "- Use Bayesian methods to estimate both individual and population-level parameters\n",
        "\n",
        "This approach produces richer insights, such as **preference heterogeneity** and **individual-level predictions**, which are critical in practical applications like **market segmentation** and **personalized recommendations**.\n"
      ],
      "id": "4b4f6157"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}